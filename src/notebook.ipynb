{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Information Extraction from Clinical Notes\n",
    "\n",
    "This notebook implements medical entity extraction and linking using Gliner, SciSpacy's UMLS and MeSH knowledge bases.\n",
    "along with relation extraction using a custom google models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Transformers version: 4.51.3\n",
      "Datasets version: 3.6.0\n",
      "SpaCy version: 3.6.1\n",
      "SciSpacy version: 0.5.5\n"
     ]
    }
   ],
   "source": [
    "# Verify environment and import dependencies\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import os\n",
    "import random\n",
    "import traceback\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "import pandas as pd\n",
    "from gliner import GLiNER\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "from functools import lru_cache\n",
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"SpaCy version: {spacy.__version__}\")\n",
    "print(f\"SciSpacy version: {scispacy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciSpacy model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunji/projects/unidlorraine/thesis/src/.venv/lib/python3.11/site-packages/spacy/language.py:2141: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/Users/tunji/projects/unidlorraine/thesis/src/.venv/lib/python3.11/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/tunji/projects/unidlorraine/thesis/src/.venv/lib/python3.11/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciSpacy pipeline loaded with components: ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner', 'abbreviation_detector', 'scispacy_linker']\n"
     ]
    }
   ],
   "source": [
    "# Load SciSpacy model with entity linker\n",
    "print(\"Loading SciSpacy model...\")\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "# Add abbreviation detector\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "\n",
    "# Add UMLS entity linker\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\n",
    "    \"resolve_abbreviations\": True,\n",
    "    \"linker_name\": \"umls\",\n",
    "    \"max_entities_per_mention\": 3  # Get top 3 candidates\n",
    "})\n",
    "\n",
    "print(\"SciSpacy pipeline loaded with components:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shuffling dataset with random seed: 224\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'AGBonnet/augmented-clinical-notes',\n",
    "    split='train',\n",
    ")\n",
    "\n",
    "def nested_print(key, element, level=0):\n",
    "    if isinstance(element, dict):\n",
    "        print(f'{\"│ \"*(level)}├─{key}:')\n",
    "        for k, v in element.items():\n",
    "            nested_print(k, v, level+1)\n",
    "    else:\n",
    "        print(f'{\"│ \"*(level)}├─{key}: {element}')\n",
    "\n",
    "# Extract idx and full_note\n",
    "def extract_idx_full_note(sample):\n",
    "    idx = sample['idx']\n",
    "    full_note = sample['full_note']\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'full_note': full_note\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(\n",
    "    extract_idx_full_note,\n",
    "    remove_columns=dataset.column_names,\n",
    "    batch_size=1000)\n",
    "\n",
    "# Shuffle dataset\n",
    "random_seed = random.randint(0, 1000)\n",
    "print(f\"\\nShuffling dataset with random seed: {random_seed}\")\n",
    "dataset = dataset.shuffle(seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1d1983a09143418ef59a9b85789352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GLiNER model for initial NER\n",
    "gliner_model = GLiNER.from_pretrained(\"Ihor/gliner-biomed-bi-large-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69297ff241546fbaa9071b5dec80ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load MedGemma model for relationship extraction\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "#TO:DO - Implement Confidence and Post-Filtering !!!!\n",
    "\n",
    "model_name = \"google/medgemma-4b-it\"\n",
    "medgemma_model, medgemma_tokenizer = load(model_name)\n",
    "use_medgemma = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad66259440b4aa0b0bdf93c029e7dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Gemma model for relationship extraction\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "gemma_model, gemma_tokenizer = load(model_name)\n",
    "use_gemma = not use_medgemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced NER function with SciSpacy entity linking\n",
    "def perform_ner_with_linking(text_note: str) -> List[Dict]:\n",
    "    \"\"\"Performs NER on text using GLiNER, then links entities using SciSpacy.\"\"\"\n",
    "\n",
    "    # Labels for the GLiNER model\n",
    "    labels = [\"Disease or Condition\", \"Medication\", \"Medication Dosage and Frequency\",\n",
    "              \"Procedure\", \"Lab Test\", \"Lab Test Result\", \"Body Site\",\n",
    "              \"Medical Device\", \"Demographic Information\"]\n",
    "\n",
    "    # Extract entities with GLiNER\n",
    "    entities = gliner_model.predict_entities(\n",
    "        text_note,\n",
    "        labels=labels,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    \n",
    "    # Process text with SciSpacy for entity linking\n",
    "    doc = nlp(text_note)\n",
    "    \n",
    "    # Create a mapping of entity text to SciSpacy linked entities\n",
    "    entity_links = {}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent._.kb_ents:\n",
    "            # Get top candidates with scores\n",
    "            candidates = []\n",
    "            for umls_ent in ent._.kb_ents[:3]:  # Top 3 candidates\n",
    "                cui, score = umls_ent\n",
    "                linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "                kb_entity = linker.kb.cui_to_entity[cui]\n",
    "                candidates.append({\n",
    "                    'cui': cui,\n",
    "                    'score': score,\n",
    "                    'name': kb_entity.canonical_name,\n",
    "                    'definition': kb_entity.definition if kb_entity.definition else '',\n",
    "                    'types': list(kb_entity.types)\n",
    "                })\n",
    "            entity_links[ent.text.lower()] = candidates\n",
    "\n",
    "    # Enhance GLiNER entities with SciSpacy linking\n",
    "    enhanced_entities = []\n",
    "    for entity in entities:\n",
    "        entity_text_lower = entity['text'].lower()\n",
    "        \n",
    "        # Check if we have linking information\n",
    "        if entity_text_lower in entity_links:\n",
    "            candidates = entity_links[entity_text_lower]\n",
    "            if candidates:\n",
    "                # Use the top candidate\n",
    "                top_candidate = candidates[0]\n",
    "                entity['cui'] = top_candidate['cui']\n",
    "                entity['canonical_name'] = top_candidate['name']\n",
    "                entity['description'] = top_candidate['definition']\n",
    "                entity['semantic_types'] = top_candidate['types']\n",
    "                entity['linking_score'] = top_candidate['score']\n",
    "                entity['alternative_candidates'] = candidates[1:] if len(candidates) > 1 else []\n",
    "\n",
    "        enhanced_entities.append(entity)\n",
    "\n",
    "    # Check for abbreviations and their expansions\n",
    "    abbreviations = {}\n",
    "    for abbr in doc._.abbreviations:\n",
    "        abbreviations[abbr.text] = abbr._.long_form.text\n",
    "    \n",
    "    # Add abbreviation information to entities\n",
    "    for entity in enhanced_entities:\n",
    "        if entity['text'] in abbreviations:\n",
    "            entity['expanded_form'] = abbreviations[entity['text']]\n",
    "\n",
    "        # Remove label and score from GLiNER entities\n",
    "        entity.pop('label', None)\n",
    "        entity.pop('score', None)\n",
    "\n",
    "    print(f\"First 5 enhanced entities: {enhanced_entities[:5]}\")\n",
    "    return enhanced_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for entity linking results\n",
    "@lru_cache(maxsize=10000)\n",
    "def cached_entity_lookup(entity_text: str) -> Dict:\n",
    "    \"\"\"Cache entity lookups to improve performance.\"\"\"\n",
    "    doc = nlp(entity_text)\n",
    "    if doc.ents and doc.ents[0]._.kb_ents:\n",
    "        ent = doc.ents[0]\n",
    "        cui, score = ent._.kb_ents[0]\n",
    "        kb_entity = nlp.kb.cui_to_entity[cui]\n",
    "        return {\n",
    "            'cui': cui,\n",
    "            'score': score,\n",
    "            'name': kb_entity.canonical_name,\n",
    "            'definition': kb_entity.definition if kb_entity.definition else ''\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex for triple extraction\n",
    "TUPLE_RX = re.compile(\n",
    "    r'''\\(\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*\\)''')\n",
    "\n",
    "def extract_triples(raw: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Return all well-formed 3-item tuples found in raw.\"\"\"\n",
    "    return [match for match in TUPLE_RX.findall(raw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced triple extraction with entity metadata\n",
    "def triplet_CIE(\n",
    "    full_note_content: str,\n",
    "    extracted_entities: List[dict],\n",
    "    max_length: int = 512,\n",
    ") -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Extract triples from a text note using MedGemma model.\"\"\"\n",
    "\n",
    "    # Format entities with their medical knowledge\n",
    "    entity_descriptions = []\n",
    "    for ent in extracted_entities:\n",
    "        desc = f\"- \\\"{ent['text']}\\\"\"\n",
    "        if 'canonical_name' in ent:\n",
    "            desc += f\" [UMLS: {ent['canonical_name']}]\"\n",
    "        if 'description' in ent and ent['description']:\n",
    "            desc += f\" - {ent['description'][:100]}...\"\n",
    "        entity_descriptions.append(desc)\n",
    "\n",
    "    entities_text = \"\\n\".join(entity_descriptions)\n",
    "\n",
    "    relationship_extraction_prompt = f\"\"\"Your goal is to perform a Closed Information Extraction task on the following clinical note:\n",
    "\n",
    "{full_note_content}\n",
    "\n",
    "You are provided with a list of medical entities extracted from the note:\n",
    "{entities_text}\n",
    "\n",
    "Your task is to generate high quality triplets of the form (entity1, relation, entity2) where:\n",
    "- The relationship is explicitly stated or strongly implied in the clinical note\n",
    "- The entities are from the provided list (use the exact text as it appears)\n",
    "- The triplets should be clinically meaningful and relevant\n",
    "\n",
    "Please return the triplets in the following format:\n",
    "[\n",
    "  (\"entity1\", \"relation\", \"entity2\"),\n",
    "  (\"entity3\", \"relation\", \"entity4\"),\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert clinical information extraction system specialized in identifying medical relationships.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": relationship_extraction_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if use_medgemma:\n",
    "        model, tokenizer = medgemma_model, medgemma_tokenizer\n",
    "    else:\n",
    "        model, tokenizer = gemma_model, gemma_tokenizer\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate text with MLX model\n",
    "    triplet_str = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=inputs,\n",
    "        verbose=False,\n",
    "        max_tokens=max_length,\n",
    "    )\n",
    "\n",
    "    triples_list = extract_triples(triplet_str)\n",
    "    \n",
    "    return triples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_entities(triples_list):\n",
    "    \"\"\"Given a list of triples, return unique entities.\"\"\"\n",
    "    entities = set()\n",
    "    for entity1, _, entity2 in triples_list:\n",
    "        entities.add(entity1)\n",
    "        entities.add(entity2)\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_query_with_metadata(entities, entity_metadata):\n",
    "    \"\"\"Generates Cypher MERGE query with UMLS metadata.\"\"\"\n",
    "    queries = []\n",
    "    \n",
    "    for i, entity in enumerate(entities):\n",
    "        query = f'MERGE (e{i}:MedicalEntity {{name: \"{entity}\"'\n",
    "        \n",
    "        # Add metadata if available\n",
    "        if entity in entity_metadata:\n",
    "            metadata = entity_metadata[entity]\n",
    "            if 'cui' in metadata:\n",
    "                query += f', cui: \"{metadata[\"cui\"]}\"'\n",
    "            if 'canonical_name' in metadata:\n",
    "                query += f', canonical_name: \"{metadata[\"canonical_name\"]}\"'\n",
    "            if 'semantic_types' in metadata:\n",
    "                types_str = '|'.join(metadata['semantic_types'])\n",
    "                query += f', semantic_types: \"{types_str}\"'\n",
    "            if 'linking_score' in metadata:\n",
    "                query += f', linking_score: {metadata[\"linking_score\"]}'\n",
    "        \n",
    "        query += '})'\n",
    "        queries.append(query)\n",
    "    \n",
    "    return '\\n'.join(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_relationships(triples_list, merge_entity_queries):\n",
    "    \"\"\"Generate Cypher MERGE statements for relationships.\"\"\"\n",
    "    # Parse entity name to variable mapping\n",
    "    entity_var_map = {}\n",
    "    for match in re.finditer(r'MERGE\\s*\\((e\\d+):MedicalEntity\\s*\\{\\s*name:\\s*\"((?:[^\"\\\\]|\\\\.)*)\"', merge_entity_queries):\n",
    "        var, name = match.group(1), match.group(2)\n",
    "        entity_var_map[name] = var\n",
    "\n",
    "    def escape_quotes(s):\n",
    "        return s.replace('\"', '\\\\\"')\n",
    "\n",
    "    def format_relation(relation):\n",
    "        return escape_quotes(relation.lower().replace(\" \", \"_\"))\n",
    "\n",
    "    seen = set()\n",
    "    cypher_lines = []\n",
    "    for entity1, relation, entity2 in triples_list:\n",
    "        var1 = entity_var_map.get(entity1)\n",
    "        var2 = entity_var_map.get(entity2)\n",
    "        if var1 and var2:\n",
    "            key = (var1, format_relation(relation), var2)\n",
    "            if key not in seen:\n",
    "                cypher_lines.append(\n",
    "                    f'MERGE ({var1})-[:RELATIONSHIP {{type: \"{format_relation(relation)}\"}}]->({var2})'\n",
    "                )\n",
    "                seen.add(key)\n",
    "    return '\\n'.join(cypher_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced parallel processing function\n",
    "def run_parallel(sample_item):\n",
    "    try:\n",
    "        index, sample_item = sample_item\n",
    "        note_id = sample_item[\"idx\"]\n",
    "        full_note_content = sample_item[\"full_note\"]\n",
    "        \n",
    "        # Perform NER with SciSpacy entity linking\n",
    "        extracted_entities = perform_ner_with_linking(full_note_content)\n",
    "\n",
    "        # Remove duplicate entities based on text\n",
    "        unique_entities = {}\n",
    "        for entity in extracted_entities:\n",
    "            key = entity['text']\n",
    "            if key not in unique_entities or \\\n",
    "               (key in unique_entities and \\\n",
    "                entity.get('linking_score', 0) > unique_entities[key].get('linking_score', 0)):\n",
    "                unique_entities[key] = entity\n",
    "\n",
    "        extracted_entities = list(unique_entities.values())\n",
    "        print(f\"Extracted {len(extracted_entities)} unique entities from note {note_id}.\")\n",
    "        \n",
    "        # Filter entities with high confidence scores\n",
    "        high_confidence_entities = [\n",
    "            ent for ent in extracted_entities \n",
    "            if ent.get('linking_score', 0) > 0.7 or 'linking_score' not in ent\n",
    "        ]\n",
    "        \n",
    "        print(f\"Filtered to {len(high_confidence_entities)} high-confidence entities.\")\n",
    "\n",
    "        # Extract relationships\n",
    "        triples_list = triplet_CIE(\n",
    "            full_note_content=full_note_content,\n",
    "            extracted_entities=high_confidence_entities,\n",
    "            max_length=512,\n",
    "        )\n",
    "        print(f\"Extracted {len(triples_list)} triplets from note {note_id}.\")\n",
    "\n",
    "        # Get unique entities from triplets\n",
    "        triple_entities = get_unique_entities(triples_list)\n",
    "        print(f\"Found {len(triple_entities)} unique entities in triplets.\")\n",
    "        \n",
    "        # Create entity metadata map\n",
    "        entity_metadata = {ent['text']: ent for ent in extracted_entities}\n",
    "        \n",
    "        # Generate Cypher queries with metadata\n",
    "        cypher_query = generate_merge_query_with_metadata(triple_entities, entity_metadata)\n",
    "        cypher_relationship_query = generate_merge_relationships(triples_list, cypher_query)\n",
    "\n",
    "        return {\n",
    "            \"note_id\": note_id,\n",
    "            \"entities\": extracted_entities,\n",
    "            \"content\": full_note_content,\n",
    "            \"triplets\": triples_list,\n",
    "            \"cypher_query\": cypher_query,\n",
    "            \"cypher_relationship_query\": cypher_relationship_query\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"note_id\": note_id, \"error\": str(e), \"traceback\": traceback.format_exc()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items to process: 1\n",
      "First 5 enhanced entities: [{'start': 317, 'end': 338, 'text': 'upper central incisor', 'cui': 'C4082814', 'canonical_name': 'Upper central incisor tooth', 'description': '', 'semantic_types': ['T023'], 'linking_score': 0.9762431979179382, 'alternative_candidates': [{'cui': 'C1840235', 'score': 0.8134047985076904, 'name': 'SOLITARY MEDIAN MAXILLARY CENTRAL INCISOR', 'definition': 'A single maxillary central incisor positioned in the midline with morphological symmetry of the crown and bordered by lateral incisors. [PMID:19125428]', 'types': ['T019']}, {'cui': 'C1827264', 'score': 0.782318115234375, 'name': 'Structure of permanent maxillary left central incisor tooth', 'definition': '<p>Permanent maxillary left central incisor tooth; Universal designation 9; ISO designation 21</p>', 'types': ['T023']}]}, {'start': 650, 'end': 659, 'text': 'incubator', 'cui': 'C0021178', 'canonical_name': 'Incubators (device)', 'description': 'Insulated enclosures in which temperature, humidity, and other environmental conditions can be regulated at levels optimal for growth, hatching, reproduction, or metabolic reactions.', 'semantic_types': ['T073'], 'linking_score': 0.9582434892654419, 'alternative_candidates': [{'cui': 'C0021179', 'score': 0.7967697381973267, 'name': 'Incubators, Infant', 'definition': 'Electrically powered devices that are intended to assist in the maintenance of the thermal balance of infants, principally by controlling the air temperature and humidity in an enclosure. (from UMDNS, 1999)', 'types': ['T074']}, {'cui': 'C0810549', 'score': 0.7967205047607422, 'name': 'Scales, Infant, Warmer/Incubator', 'definition': 'Infant scales designed for high-precision and high-accuracy weighing of infants while they are either in a warmer or an incubator. These scales usually consist of a weighing platform with a transducer that is placed under the mattress and a separate electronic weighing module that includes the display; they may also include a tray and a mattress, depending on the warmer or incubator used. Most of these scales have a weighing capacity of up to 8 to 15 kg (18 to 33 lb), with an accuracy of 5 g (0.0125 lb). Warmer/incubator infant scales are mostly used for continuous monitoring of weight in critical care situations when highly accurate weighing is needed (e.g., water to monitor losses in low-birth-weight neonates, to detect dehydration or overhydration during pediatric hemodialysis).', 'types': ['T074']}]}, {'start': 837, 'end': 858, 'text': 'intraoral examination', 'cui': 'C0376306', 'canonical_name': 'Oral Examination', 'description': 'A systematic evaluation of the tissues and structures in the mouth.', 'semantic_types': ['T058'], 'linking_score': 0.7816542983055115, 'alternative_candidates': [{'cui': 'C0750862', 'score': 0.7816542983055115, 'name': 'Dental examination', 'definition': 'A systematic evaluation of the mouth, face, and neck, which may include tooth counting, cleaning and visual assessments for abnormalities such as caries, gum disease, oral cancer, and structural malformations.', 'types': ['T060']}, {'cui': 'C0031809', 'score': 0.7129306793212891, 'name': 'Physical Examination', 'definition': 'Systematic and thorough inspection of the patient for physical signs of disease or abnormality.', 'types': ['T058']}]}, {'start': 871, 'end': 892, 'text': 'mixed dentition stage', 'cui': 'C0011444', 'canonical_name': 'Dentition, Mixed', 'description': \"The complement of teeth in the jaws after the eruption of some of the permanent teeth but before all the deciduous teeth are absent. (Boucher's Clinical Dental Terminology, 4th ed)\", 'semantic_types': ['T023'], 'linking_score': 0.805463433265686, 'alternative_candidates': []}, {'start': 1306, 'end': 1327, 'text': 'hypoplastic phenotype'}]\n",
      "Extracted 19 unique entities from note 28616.\n",
      "Filtered to 19 high-confidence entities.\n",
      "Extracted 26 triplets from note 28616.\n",
      "Found 6 unique entities in triplets.\n",
      "Processing complete. Check data/medgemma_224_enhanced_extraction_results.txt for results.\n",
      "SciSpacy entity linking cache size: CacheInfo(hits=0, misses=0, maxsize=10000, currsize=0)\n"
     ]
    }
   ],
   "source": [
    "# Main execution cell\n",
    "import concurrent.futures\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "file_path = \"data/\" + (\"medgemma_\" if use_medgemma else \"gemma_\") + str(random_seed) + \"_enhanced_extraction_results.txt\"\n",
    "\n",
    "# Clear previous contents\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(\"=\"*30)\n",
    "    f.write(\" Enhanced Extraction Process using \" + use_medgemma * \"MedGemma \" if use_medgemma else \"Gemma \" + \"Model\")\n",
    "    f.write(\"=\"*30 + \"\\n\\n\")\n",
    "\n",
    "# Convert dataset to list for processing\n",
    "items_to_process = list(enumerate(dataset))\n",
    "\n",
    "# Process first few items for testing\n",
    "items_to_process = items_to_process[:1]  # Adjust based on compute resources\n",
    "\n",
    "print(f\"Total items to process: {len(items_to_process)}\")\n",
    "\n",
    "# Execute in parallel with optimized batching\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    ner_results = list(executor.map(run_parallel, items_to_process))\n",
    "\n",
    "# Write enhanced results to file\n",
    "with open(file_path, \"a\") as f:\n",
    "    for result in ner_results:\n",
    "        if \"error\" in result:\n",
    "            f.write(f\"Error processing note {result['note_id']}: {result['error']}\\n\")\n",
    "            f.write(result['traceback'] + \"\\n\")\n",
    "        else:\n",
    "            f.write(f\"Note ID: {result['note_id']} (Seed: {random_seed})\\n\")\n",
    "            f.write(\"\\nContent:\\n\")\n",
    "            f.write(result['content'] + \"\\n\\n\")\n",
    "            f.write(\"Entities with UMLS Linking:\\n\")\n",
    "            \n",
    "            for entity in result['entities']:\n",
    "                f.write(f\"  - Text: {entity['text']}\\n\")\n",
    "\n",
    "                if 'cui' in entity:\n",
    "                    f.write(f\"    CUI: {entity['cui']}\\n\")\n",
    "                    f.write(f\"    Canonical Name: {entity.get('canonical_name', 'N/A')}\\n\")\n",
    "                    f.write(f\"    Linking Score: {entity.get('linking_score', 'N/A'):.3f}\\n\")\n",
    "\n",
    "                if 'semantic_types' in entity:\n",
    "                    f.write(f\"    Semantic Types: {', '.join(entity['semantic_types'])}\\n\")\n",
    "                \n",
    "                if 'description' in entity and entity['description']:\n",
    "                    f.write(f\"    Description: {entity['description'][:150]}...\\n\")\n",
    "                \n",
    "                if 'expanded_form' in entity:\n",
    "                    f.write(f\"    Expanded Form: {entity['expanded_form']}\\n\")\n",
    "\n",
    "                if 'alternative_candidates' in entity and entity['alternative_candidates']:\n",
    "                    f.write(\"    Alternative Candidates:\\n\")\n",
    "                    for alt in entity['alternative_candidates'][:2]:\n",
    "                        f.write(f\"      - {alt['name']} (CUI: {alt['cui']}, Score: {alt['score']:.3f})\\n\")\n",
    "            \n",
    "            f.write(\"\\nTriplets:\\n\")\n",
    "            for triplet in result['triplets']:\n",
    "                f.write(f\"  - ({triplet[0]}, {triplet[1]}, {triplet[2]})\\n\")\n",
    "            \n",
    "            f.write(\"\\nCypher Query with Metadata:\\n\")\n",
    "            f.write(result['cypher_query'] + \"\\n\\n\")\n",
    "            f.write(\"Cypher Relationship Query:\\n\")\n",
    "            f.write(result['cypher_relationship_query'] + \"\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Processing complete. Check {file_path} for results.\")\n",
    "print(f\"SciSpacy entity linking cache size: {cached_entity_lookup.cache_info()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
