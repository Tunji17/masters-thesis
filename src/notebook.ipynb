{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Information Extraction from Clinical Notes\n",
    "\n",
    "This notebook implements medical entity extraction and linking using Gliner, SciSpacy's UMLS and MeSH knowledge bases.\n",
    "along with relation extraction using a custom google models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Transformers version: 4.51.3\n",
      "Datasets version: 3.6.0\n",
      "SpaCy version: 3.6.1\n",
      "SciSpacy version: 0.5.5\n"
     ]
    }
   ],
   "source": [
    "# Verify environment and import dependencies\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "import os\n",
    "import random\n",
    "import traceback\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.linking import EntityLinker\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "import pandas as pd\n",
    "from gliner import GLiNER\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "from functools import lru_cache\n",
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "print(f\"SpaCy version: {spacy.__version__}\")\n",
    "print(f\"SciSpacy version: {scispacy.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciSpacy model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunji/projects/unidlorraine/thesis/src/.venv/lib/python3.11/site-packages/spacy/language.py:2141: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "/Users/tunji/projects/unidlorraine/thesis/src/.venv/lib/python3.11/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.1.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/Users/tunji/projects/unidlorraine/thesis/src/.venv/lib/python3.11/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.1.2 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciSpacy pipeline loaded with components: ['tok2vec', 'tagger', 'attribute_ruler', 'lemmatizer', 'parser', 'ner', 'abbreviation_detector', 'scispacy_linker']\n"
     ]
    }
   ],
   "source": [
    "# Load SciSpacy model with entity linker\n",
    "print(\"Loading SciSpacy model...\")\n",
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "# Add abbreviation detector\n",
    "nlp.add_pipe(\"abbreviation_detector\")\n",
    "\n",
    "# Add UMLS entity linker\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\n",
    "    \"resolve_abbreviations\": True,\n",
    "    \"linker_name\": \"umls\",\n",
    "    \"max_entities_per_mention\": 3  # Get top 3 candidates\n",
    "})\n",
    "\n",
    "print(\"SciSpacy pipeline loaded with components:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shuffling dataset with random seed: 650\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'AGBonnet/augmented-clinical-notes',\n",
    "    split='train',\n",
    ")\n",
    "\n",
    "def nested_print(key, element, level=0):\n",
    "    if isinstance(element, dict):\n",
    "        print(f'{\"│ \"*(level)}├─{key}:')\n",
    "        for k, v in element.items():\n",
    "            nested_print(k, v, level+1)\n",
    "    else:\n",
    "        print(f'{\"│ \"*(level)}├─{key}: {element}')\n",
    "\n",
    "# Extract idx and full_note\n",
    "def extract_idx_full_note(sample):\n",
    "    idx = sample['idx']\n",
    "    full_note = sample['full_note']\n",
    "    return {\n",
    "        'idx': idx,\n",
    "        'full_note': full_note\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(\n",
    "    extract_idx_full_note,\n",
    "    remove_columns=dataset.column_names,\n",
    "    batch_size=1000)\n",
    "\n",
    "# Shuffle dataset\n",
    "random_seed = random.randint(0, 1000)\n",
    "print(f\"\\nShuffling dataset with random seed: {random_seed}\")\n",
    "dataset = dataset.shuffle(seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f47e9cc57c4105b8b8552e1304e7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load GLiNER model for initial NER\n",
    "gliner_model = GLiNER.from_pretrained(\"Ihor/gliner-biomed-bi-large-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f7c230f2894240a7c61b4ff2f659d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load MedGemma model for relationship extraction\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "#TO:DO - Implement Confidence and Post-Filtering !!!!\n",
    "\n",
    "model_name = \"google/medgemma-4b-it\"\n",
    "medgemma_model, medgemma_tokenizer = load(model_name)\n",
    "use_medgemma = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4276ce729c44f78ff34fc5884d5f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Gemma model for relationship extraction\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "gemma_model, gemma_tokenizer = load(model_name)\n",
    "use_gemma = not use_medgemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced NER function with SciSpacy entity linking\n",
    "def perform_ner_with_linking(text_note: str) -> List[Dict]:\n",
    "    \"\"\"Performs NER on text using GLiNER, then links entities using SciSpacy.\"\"\"\n",
    "\n",
    "    # Labels for the GLiNER model\n",
    "    labels = [\"Disease or Condition\", \"Medication\", \"Medication Dosage and Frequency\",\n",
    "              \"Procedure\", \"Lab Test\", \"Lab Test Result\", \"Body Site\",\n",
    "              \"Medical Device\", \"Demographic Information\"]\n",
    "\n",
    "    # Extract entities with GLiNER\n",
    "    entities = gliner_model.predict_entities(\n",
    "        text_note,\n",
    "        labels=labels,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    \n",
    "    # Process text with SciSpacy for entity linking\n",
    "    doc = nlp(text_note)\n",
    "    \n",
    "    # Create a mapping of entity text to SciSpacy linked entities\n",
    "    entity_links = {}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent._.kb_ents:\n",
    "            # Get top candidates with scores\n",
    "            candidates = []\n",
    "            for umls_ent in ent._.kb_ents[:3]:  # Top 3 candidates\n",
    "                cui, score = umls_ent\n",
    "                linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "                kb_entity = linker.kb.cui_to_entity[cui]\n",
    "                candidates.append({\n",
    "                    'cui': cui,\n",
    "                    'score': score,\n",
    "                    'name': kb_entity.canonical_name,\n",
    "                    'definition': kb_entity.definition if kb_entity.definition else '',\n",
    "                    'types': list(kb_entity.types)\n",
    "                })\n",
    "            entity_links[ent.text.lower()] = candidates\n",
    "\n",
    "    # Enhance GLiNER entities with SciSpacy linking\n",
    "    enhanced_entities = []\n",
    "    for entity in entities:\n",
    "        entity_text_lower = entity['text'].lower()\n",
    "        \n",
    "        # Check if we have linking information\n",
    "        if entity_text_lower in entity_links:\n",
    "            candidates = entity_links[entity_text_lower]\n",
    "            if candidates:\n",
    "                # Use the top candidate\n",
    "                top_candidate = candidates[0]\n",
    "                entity['cui'] = top_candidate['cui']\n",
    "                entity['canonical_name'] = top_candidate['name']\n",
    "                entity['description'] = top_candidate['definition']\n",
    "                entity['semantic_types'] = top_candidate['types']\n",
    "                entity['linking_score'] = top_candidate['score']\n",
    "                entity['alternative_candidates'] = candidates[1:] if len(candidates) > 1 else []\n",
    "\n",
    "        enhanced_entities.append(entity)\n",
    "\n",
    "    # Check for abbreviations and their expansions\n",
    "    abbreviations = {}\n",
    "    for abbr in doc._.abbreviations:\n",
    "        abbreviations[abbr.text] = abbr._.long_form.text\n",
    "    \n",
    "    # Add abbreviation information to entities\n",
    "    for entity in enhanced_entities:\n",
    "        if entity['text'] in abbreviations:\n",
    "            entity['expanded_form'] = abbreviations[entity['text']]\n",
    "\n",
    "        # Remove label and score from GLiNER entities\n",
    "        entity.pop('label', None)\n",
    "        entity.pop('score', None)\n",
    "\n",
    "    print(f\"First 5 enhanced entities: {enhanced_entities[:5]}\")\n",
    "    return enhanced_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for entity linking results\n",
    "@lru_cache(maxsize=10000)\n",
    "def cached_entity_lookup(entity_text: str) -> Dict:\n",
    "    \"\"\"Cache entity lookups to improve performance.\"\"\"\n",
    "    doc = nlp(entity_text)\n",
    "    if doc.ents and doc.ents[0]._.kb_ents:\n",
    "        ent = doc.ents[0]\n",
    "        cui, score = ent._.kb_ents[0]\n",
    "        kb_entity = nlp.kb.cui_to_entity[cui]\n",
    "        return {\n",
    "            'cui': cui,\n",
    "            'score': score,\n",
    "            'name': kb_entity.canonical_name,\n",
    "            'definition': kb_entity.definition if kb_entity.definition else ''\n",
    "        }\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex for triple extraction\n",
    "TUPLE_RX = re.compile(\n",
    "    r'''\\(\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*\\)''')\n",
    "\n",
    "def extract_triples(raw: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Return all well-formed 3-item tuples found in raw.\"\"\"\n",
    "    return [match for match in TUPLE_RX.findall(raw)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced triple extraction with entity metadata\n",
    "def triplet_CIE(\n",
    "    full_note_content: str,\n",
    "    extracted_entities: List[dict],\n",
    "    max_length: int = 512,\n",
    ") -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Extract triples from a text note using MedGemma model.\"\"\"\n",
    "\n",
    "    # Format entities with their medical knowledge\n",
    "    entity_descriptions = []\n",
    "    for ent in extracted_entities:\n",
    "        desc = f\"- \\\"{ent['text']}\\\"\"\n",
    "        if 'canonical_name' in ent:\n",
    "            desc += f\" [UMLS: {ent['canonical_name']}]\"\n",
    "        if 'description' in ent and ent['description']:\n",
    "            desc += f\" - {ent['description'][:100]}...\"\n",
    "        entity_descriptions.append(desc)\n",
    "\n",
    "    entities_text = \"\\n\".join(entity_descriptions)\n",
    "\n",
    "    relationship_extraction_prompt = f\"\"\"Your goal is to perform a Closed Information Extraction task on the following clinical note:\n",
    "\n",
    "{full_note_content}\n",
    "\n",
    "You are provided with a list of medical entities extracted from the note:\n",
    "{entities_text}\n",
    "\n",
    "Your task is to generate high quality triplets of the form (entity1, relation, entity2) where:\n",
    "- The relationship is explicitly stated or strongly implied in the clinical note\n",
    "- The entities are from the provided list (use the exact text as it appears)\n",
    "- The triplets should be clinically meaningful and relevant\n",
    "\n",
    "Please return the triplets in the following format:\n",
    "[\n",
    "  (\"entity1\", \"relation\", \"entity2\"),\n",
    "  (\"entity3\", \"relation\", \"entity4\"),\n",
    "  ...\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an expert clinical information extraction system specialized in identifying medical relationships.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": relationship_extraction_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if use_medgemma:\n",
    "        model, tokenizer = medgemma_model, medgemma_tokenizer\n",
    "    else:\n",
    "        model, tokenizer = gemma_model, gemma_tokenizer\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Generate text with MLX model\n",
    "    triplet_str = generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt=inputs,\n",
    "        verbose=False,\n",
    "        max_tokens=max_length,\n",
    "    )\n",
    "\n",
    "    triples_list = extract_triples(triplet_str)\n",
    "    \n",
    "    return triples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_entities(triples_list):\n",
    "    \"\"\"Given a list of triples, return unique entities.\"\"\"\n",
    "    entities = set()\n",
    "    for entity1, _, entity2 in triples_list:\n",
    "        entities.add(entity1)\n",
    "        entities.add(entity2)\n",
    "    return list(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_query_with_metadata(entities, entity_metadata):\n",
    "    \"\"\"Generates Cypher MERGE query with UMLS metadata.\"\"\"\n",
    "    queries = []\n",
    "    \n",
    "    for i, entity in enumerate(entities):\n",
    "        query = f'MERGE (e{i}:MedicalEntity {{name: \"{entity}\"'\n",
    "        \n",
    "        # Add metadata if available\n",
    "        if entity in entity_metadata:\n",
    "            metadata = entity_metadata[entity]\n",
    "            if 'cui' in metadata:\n",
    "                query += f', cui: \"{metadata[\"cui\"]}\"'\n",
    "            if 'canonical_name' in metadata:\n",
    "                query += f', canonical_name: \"{metadata[\"canonical_name\"]}\"'\n",
    "            if 'semantic_types' in metadata:\n",
    "                types_str = '|'.join(metadata['semantic_types'])\n",
    "                query += f', semantic_types: \"{types_str}\"'\n",
    "            if 'linking_score' in metadata:\n",
    "                query += f', linking_score: {metadata[\"linking_score\"]}'\n",
    "        \n",
    "        query += '})'\n",
    "        queries.append(query)\n",
    "    \n",
    "    return '\\n'.join(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_merge_relationships(triples_list, merge_entity_queries):\n",
    "    \"\"\"Generate Cypher MERGE statements for relationships.\"\"\"\n",
    "    # Parse entity name to variable mapping\n",
    "    entity_var_map = {}\n",
    "    for match in re.finditer(r'MERGE\\s*\\((e\\d+):MedicalEntity\\s*\\{\\s*name:\\s*\"((?:[^\"\\\\]|\\\\.)*)\"', merge_entity_queries):\n",
    "        var, name = match.group(1), match.group(2)\n",
    "        entity_var_map[name] = var\n",
    "\n",
    "    def escape_quotes(s):\n",
    "        return s.replace('\"', '\\\\\"')\n",
    "\n",
    "    def format_relation(relation):\n",
    "        return escape_quotes(relation.lower().replace(\" \", \"_\"))\n",
    "\n",
    "    seen = set()\n",
    "    cypher_lines = []\n",
    "    for entity1, relation, entity2 in triples_list:\n",
    "        var1 = entity_var_map.get(entity1)\n",
    "        var2 = entity_var_map.get(entity2)\n",
    "        if var1 and var2:\n",
    "            key = (var1, format_relation(relation), var2)\n",
    "            if key not in seen:\n",
    "                cypher_lines.append(\n",
    "                    f'MERGE ({var1})-[:RELATIONSHIP {{type: \"{format_relation(relation)}\"}}]->({var2})'\n",
    "                )\n",
    "                seen.add(key)\n",
    "    return '\\n'.join(cypher_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced parallel processing function\n",
    "def run_parallel(sample_item):\n",
    "    try:\n",
    "        index, sample_item = sample_item\n",
    "        note_id = sample_item[\"idx\"]\n",
    "        full_note_content = sample_item[\"full_note\"]\n",
    "        \n",
    "        # Perform NER with SciSpacy entity linking\n",
    "        extracted_entities = perform_ner_with_linking(full_note_content)\n",
    "\n",
    "        # Remove duplicate entities based on text\n",
    "        unique_entities = {}\n",
    "        for entity in extracted_entities:\n",
    "            key = entity['text']\n",
    "            if key not in unique_entities or \\\n",
    "               (key in unique_entities and \\\n",
    "                entity.get('linking_score', 0) > unique_entities[key].get('linking_score', 0)):\n",
    "                unique_entities[key] = entity\n",
    "\n",
    "        extracted_entities = list(unique_entities.values())\n",
    "        print(f\"Extracted {len(extracted_entities)} unique entities from note {note_id}.\")\n",
    "        \n",
    "        # Filter entities with high confidence scores\n",
    "        high_confidence_entities = [\n",
    "            ent for ent in extracted_entities \n",
    "            if ent.get('linking_score', 0) > 0.7 or 'linking_score' not in ent\n",
    "        ]\n",
    "        \n",
    "        print(f\"Filtered to {len(high_confidence_entities)} high-confidence entities.\")\n",
    "\n",
    "        # Extract relationships\n",
    "        triples_list = triplet_CIE(\n",
    "            full_note_content=full_note_content,\n",
    "            extracted_entities=high_confidence_entities,\n",
    "            max_length=512,\n",
    "        )\n",
    "        print(f\"Extracted {len(triples_list)} triplets from note {note_id}.\")\n",
    "\n",
    "        # Get unique entities from triplets\n",
    "        triple_entities = get_unique_entities(triples_list)\n",
    "        print(f\"Found {len(triple_entities)} unique entities in triplets.\")\n",
    "        \n",
    "        # Create entity metadata map\n",
    "        entity_metadata = {ent['text']: ent for ent in extracted_entities}\n",
    "        \n",
    "        # Generate Cypher queries with metadata\n",
    "        cypher_query = generate_merge_query_with_metadata(triple_entities, entity_metadata)\n",
    "        cypher_relationship_query = generate_merge_relationships(triples_list, cypher_query)\n",
    "\n",
    "        return {\n",
    "            \"note_id\": note_id,\n",
    "            \"entities\": extracted_entities,\n",
    "            \"content\": full_note_content,\n",
    "            \"triplets\": triples_list,\n",
    "            \"cypher_query\": cypher_query,\n",
    "            \"cypher_relationship_query\": cypher_relationship_query\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"note_id\": note_id, \"error\": str(e), \"traceback\": traceback.format_exc()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items to process: 1\n",
      "First 5 enhanced entities: [{'start': 87, 'end': 112, 'text': 'native aortic coarctation'}, {'start': 316, 'end': 334, 'text': 'right and left arm'}, {'start': 377, 'end': 397, 'text': 'right and left ankle'}, {'start': 433, 'end': 450, 'text': 'upper extremities', 'cui': 'C1140618', 'canonical_name': 'Upper Extremity', 'description': 'The region of the upper limb in animals, extending from the deltoid region to the HAND, and including the ARM; AXILLA; and SHOULDER.', 'semantic_types': ['T023'], 'linking_score': 0.9906526207923889, 'alternative_candidates': [{'cui': 'C0003793', 'score': 0.8803924322128296, 'name': 'Bone structure of upper limb', 'definition': 'The bones of the upper and lower ARM. They include the CLAVICLE and SCAPULA.', 'types': ['T023']}, {'cui': 'C0222201', 'score': 0.8537867069244385, 'name': 'Skin structure of upper limb', 'definition': 'The integumentary covering of the upper extremities.', 'types': ['T023']}]}, {'start': 458, 'end': 481, 'text': 'echocardiographic study', 'cui': 'C0013798', 'canonical_name': 'Electrocardiogram', 'description': 'A study that evaluates the effect of a treatment on cardiac electrical activity, as assessed by electrocardiography.', 'semantic_types': ['T060'], 'linking_score': 0.7707390189170837, 'alternative_candidates': [{'cui': 'C0183129', 'score': 0.7509099841117859, 'name': 'Echocardiographs', 'definition': 'An echocardiograph is a device that uses ultrasonic energy to create images of cardiovascular structures. It includes phased arrays and two-dimensional scanners.', 'types': ['T074']}, {'cui': 'C0013516', 'score': 0.7492713332176208, 'name': 'Echocardiography', 'definition': 'Ultrasonic recording of the size, motion, and composition of the heart and surrounding tissues. The standard approach is transthoracic.', 'types': ['T060']}]}]\n",
      "Extracted 21 unique entities from note 161157.\n",
      "Filtered to 21 high-confidence entities.\n",
      "Extracted 24 triplets from note 161157.\n",
      "Found 2 unique entities in triplets.\n",
      "Processing complete. Check data/medgemma_650_enhanced_extraction_results.txt for results.\n",
      "SciSpacy entity linking cache size: CacheInfo(hits=0, misses=0, maxsize=10000, currsize=0)\n"
     ]
    }
   ],
   "source": [
    "# Main execution cell\n",
    "import concurrent.futures\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "file_path = \"data/\" + (\"medgemma_\" if use_medgemma else \"gemma_\") + str(random_seed) + \"_enhanced_extraction_results.txt\"\n",
    "\n",
    "# Clear previous contents\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(\"=\"*30)\n",
    "    f.write(\" Enhanced Extraction Process using \" + use_medgemma * \"MedGemma \" if use_medgemma else \"Gemma \" + \"Model\")\n",
    "    f.write(\"=\"*30 + \"\\n\\n\")\n",
    "\n",
    "# Convert dataset to list for processing\n",
    "items_to_process = list(enumerate(dataset))\n",
    "\n",
    "# Process first few items for testing\n",
    "items_to_process = items_to_process[:1]  # Adjust based on compute resources\n",
    "\n",
    "print(f\"Total items to process: {len(items_to_process)}\")\n",
    "\n",
    "# Execute in parallel with optimized batching\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    ner_results = list(executor.map(run_parallel, items_to_process))\n",
    "\n",
    "# Write enhanced results to file\n",
    "with open(file_path, \"a\") as f:\n",
    "    for result in ner_results:\n",
    "        if \"error\" in result:\n",
    "            f.write(f\"Error processing note {result['note_id']}: {result['error']}\\n\")\n",
    "            f.write(result['traceback'] + \"\\n\")\n",
    "        else:\n",
    "            f.write(f\"Note ID: {result['note_id']} (Seed: {random_seed})\\n\")\n",
    "            f.write(\"\\nContent:\\n\")\n",
    "            f.write(result['content'] + \"\\n\\n\")\n",
    "            f.write(\"Entities with UMLS Linking:\\n\")\n",
    "            \n",
    "            for entity in result['entities']:\n",
    "                f.write(f\"  - Text: {entity['text']}\\n\")\n",
    "\n",
    "                if 'cui' in entity:\n",
    "                    f.write(f\"    CUI: {entity['cui']}\\n\")\n",
    "                    f.write(f\"    Canonical Name: {entity.get('canonical_name', 'N/A')}\\n\")\n",
    "                    f.write(f\"    Linking Score: {entity.get('linking_score', 'N/A'):.3f}\\n\")\n",
    "\n",
    "                if 'semantic_types' in entity:\n",
    "                    f.write(f\"    Semantic Types: {', '.join(entity['semantic_types'])}\\n\")\n",
    "                \n",
    "                if 'description' in entity and entity['description']:\n",
    "                    f.write(f\"    Description: {entity['description'][:150]}...\\n\")\n",
    "                \n",
    "                if 'expanded_form' in entity:\n",
    "                    f.write(f\"    Expanded Form: {entity['expanded_form']}\\n\")\n",
    "\n",
    "                if 'alternative_candidates' in entity and entity['alternative_candidates']:\n",
    "                    f.write(\"    Alternative Candidates:\\n\")\n",
    "                    for alt in entity['alternative_candidates'][:2]:\n",
    "                        f.write(f\"      - {alt['name']} (CUI: {alt['cui']}, Score: {alt['score']:.3f})\\n\")\n",
    "            \n",
    "            f.write(\"\\nTriplets:\\n\")\n",
    "            for triplet in result['triplets']:\n",
    "                f.write(f\"  - ({triplet[0]}, {triplet[1]}, {triplet[2]})\\n\")\n",
    "            \n",
    "            f.write(\"\\nCypher Query with Metadata:\\n\")\n",
    "            f.write(result['cypher_query'] + \"\\n\\n\")\n",
    "            f.write(\"Cypher Relationship Query:\\n\")\n",
    "            f.write(result['cypher_relationship_query'] + \"\\n\")\n",
    "            f.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "print(f\"Processing complete. Check {file_path} for results.\")\n",
    "print(f\"SciSpacy entity linking cache size: {cached_entity_lookup.cache_info()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
