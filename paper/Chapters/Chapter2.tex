% Chapter 2

\chapter{Literature Review and Theoretical Background}
\label{chap:literature}

\section{Natural Language Processing in Healthcare}

\subsection{Evolution of Clinical NLP}

Natural language processing (NLP) techniques have been applied to clinical text for several decades. Early clinical NLP systems often relied on \textbf{rule-based} or \textbf{dictionary-driven} approaches. These systems used handcrafted rules and lookup lists of medical terms to extract information from clinical narratives. For example, an early system might flag any capitalized word followed by "disease" as a diagnosis, or use a dictionary of drug names to identify medications. Such rule-based methods, including those integrated into early clinical NLP tools (e.g., the Mayo Clinic's cTAKES or NLM's MetaMap), could achieve high precision on well-defined patterns. However, they were brittle – coverage was limited to what the rules enumerated, and maintaining a large set of rules for evolving clinical language proved labor-intensive.

By the late 1990s and 2000s, \textbf{statistical machine learning} approaches began to emerge in clinical NLP. Researchers started formulating problems like entity recognition or relation extraction as supervised learning tasks, moving away from purely manual rules. For instance, classification algorithms (Support Vector Machines, Maximum Entropy models, etc.) were trained on annotated clinical corpora to detect entities or relationships. Sequence labeling methods such as Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) became popular for tasks like \textbf{clinical named entity recognition (NER)}. These models use probabilities learned from data rather than fixed rules, allowing more flexibility. A CRF model could learn that "fever" or "pain" are likely problem entities if they occur in certain contexts, without an explicit dictionary. This statistical era greatly improved recall (catching more valid variations) but still required careful feature engineering by experts (e.g., designing features for medical abbreviations, context window words, part-of-speech tags, etc.).

Over the last decade, the field has undergone a \textbf{deep learning revolution}. Neural network–based models, which can automatically learn features from large amounts of text, have achieved state-of-the-art results in clinical NLP. Initial forays used \textbf{recurrent neural networks} (RNNs) or \textbf{convolutional neural networks} (CNNs) to process clinical text. For example, a bidirectional LSTM network with a CRF output layer can learn to recognize medical entities by capturing character-level and word-level patterns, surpassing the performance of CRF with manual features. The introduction of \textbf{word embeddings} (dense vector representations of words trained on unlabeled text) also boosted performance – models could leverage semantic similarities (e.g., that "myocardial" is similar to "cardiac") without explicit rules. By the late 2010s, the \textbf{transformer architecture} and in particular BERT-like models brought another leap. These models, pretrained on vast text corpora, allowed fine-tuning on relatively small clinical datasets with excellent results. The evolution of clinical NLP can thus be seen in phases: from rigid rule-based systems, to statistical ML with feature engineering, and now to end-to-end deep learning approaches. This progression has dramatically improved the ability to process free-text clinical notes, though challenges like data privacy, integration into workflows, and limited labeled data remain obstacles to broad deployment.

\subsection{Current State-of-the-Art Approaches}

State-of-the-art (SOTA) NLP in healthcare today is dominated by \textbf{advanced neural models}, especially transformer-based language models. In the realm of clinical text, researchers now routinely use \textbf{domain-specific pretrained models} to boost performance. For example, \textbf{BioBERT}, \textbf{ClinicalBERT}, \textbf{SciBERT}, and related models take the BERT architecture and continue pretraining it on biomedical literature or clinical note corpora. These models have learned the nuances of medical terminology from millions of words of domain text. As a result, they significantly outperform generic NLP models on tasks like medical NER and question answering. One study noted that BioBERT (trained on PubMed abstracts and PMC articles) \textbf{"significantly outperformed vanilla BERT on core biomedical tasks such as named entity recognition (NER), relation extraction, and question answering."} This has made biomedical transformers a de facto starting point for most new studies in clinical NLP.

In parallel, the NLP field at large has seen the rise of \textbf{large language models (LLMs)}, and these are beginning to influence clinical NLP. Models like GPT-3, GPT-4, PaLM, and others with tens or hundreds of billions of parameters have demonstrated impressive capabilities to understand and generate text. Researchers have started applying LLMs to clinical problems – for instance, prompting an LLM to extract medical information without task-specific training. Recent findings suggest that LLMs can achieve strong results in biomedical information extraction even in \textbf{zero-shot or few-shot settings}. An example is using GPT-4 to parse a radiology report and list key findings, or classify the relationship between a drug and an adverse effect in a sentence by just giving the model a suitable prompt. These models leverage the broad knowledge encoded during pretraining (some have seen portions of medical literature or general health information on the web). While not specialized to clinical text, their sheer scale often compensates, and fine-tuning or instructing them on biomedical tasks can yield high accuracy. It is now state-of-the-art to explore \emph{hybrid approaches}: using domain-specific models for certain tasks and general LLMs for others, or comparing their performance.

Despite the excitement around transformers and LLMs, practical state-of-the-art systems often combine multiple components and techniques. For example, a top-performing pipeline for clinical note understanding might use a \textbf{biomedical NER model} to find entities, a \textbf{rule-based or dictionary method} to handle specific patterns (e.g., lab values or abbreviations), and a \textbf{transformer-based relation extractor} to link entities. Integration with clinical knowledge bases (like UMLS) is also common in SOTA systems to ground extracted text in standardized medical concepts. In summary, current best approaches in clinical NLP leverage (1) pretrained biomedical language models for core NLP tasks, (2) large general models for generative or zero-shot capabilities, and (3) domain knowledge (ontologies, dictionaries, rules) to handle the idiosyncrasies of clinical data. This thesis builds upon these state-of-the-art trends, employing specialized models and knowledge resources to maximize extraction performance from clinical text.

\section{Named Entity Recognition in Medical Text}

\subsection{Traditional NER Methods}

\textbf{Named Entity Recognition (NER)} in medical text involves identifying spans of text that correspond to clinical concepts such as diseases, symptoms, medications, or procedures. Traditional approaches to medical NER were dominated by \textbf{rule-based and dictionary-based methods}. In a dictionary-based method, one compiles extensive lists of known entity names for each category (for example, a list of all drug names, a list of all human anatomy terms, etc.) and then scans the text for exact or fuzzy matches. Many early clinical NLP systems took this approach, often augmented with simple variants (e.g., allowing minor spelling differences or acronyms). Rule-based methods, on the other hand, rely on human-crafted linguistic patterns. For instance, a rule might be: "if the token 'diagnosed with' appears, the following words until a comma or verb are a Disease entity." Such rules leverage the observation that certain syntactic contexts reliably indicate an entity. \textbf{cTAKES}, an open-source clinical text processing tool, began with a heavy reliance on dictionary lookups (using the UMLS thesaurus) to identify entities, essentially a dictionary-based NER. Similarly, \textbf{MetaMap} (developed by the National Library of Medicine) uses a combination of dictionary lookup and lexical variation generation to map text to medical concepts, thereby performing entity recognition.

These traditional methods often achieved \textbf{high precision} in identifying entities that exactly match their lists or patterns – a drug dictionary will rarely misidentify a common word as a drug. However, they showed \textbf{limited recall and poor adaptability}. New or uncommon terminology, or mentions phrased in unexpected ways, would be missed. For example, a dictionary might contain "myocardial infarction" but not catch an abbreviated slang like "MI" unless explicitly added. Rule-based systems similarly struggle with the variability of clinical language; a slight change in wording can break a rule. Maintaining and updating these NER systems was difficult, as medical vocabulary continuously evolves (new drug names, emerging diseases, etc.) and language use varies across institutions and clinicians. Another challenge was \textbf{ambiguity} – many medical terms can play multiple roles. A rule-based system might mistakenly label "June" as a Person because months weren't anticipated, or label "mass" as a symptom when it might be a measurement unit, without more complex understanding.

Nonetheless, traditional NER methods laid the foundation and are still in use in constrained scenarios. They are \textbf{interpretable} (one can see why a term was recognized) and can be quickly deployed if one has a decent lexicon. In low-resource settings or specific subdomains (e.g., a hospital might maintain a custom list of its clinic names to pick out), these methods remain valuable. They also complement modern methods as a fallback or an ensemble component, because their precision can help validate or filter the more "eager" neural model outputs. Overall, however, the field has largely moved beyond purely rule-based NER due to the labor required and the need for higher recall and robustness.

\subsection{Deep Learning Approaches for Medical NER}

Deep learning approaches revolutionized medical NER by removing the need for manual feature engineering and by vastly improving the ability to generalize to new examples. Around the mid-2010s, researchers began applying \textbf{neural network models} to biomedical NER tasks. One common architecture was the \textbf{BiLSTM-CRF}: a bidirectional Long Short-Term Memory network feeding into a Conditional Random Field output layer. In this setup, word sequences are fed into the LSTM which captures context from both left and right (crucial for clinical text where context determines entity boundaries), and then a CRF layer jointly decodes the best sequence of labels (ensuring a consistent output like "B-problem, I-problem, O"). Such models quickly proved effective, outperforming earlier CRF models that relied on hand-crafted features. Studies found that incorporating \textbf{character-level CNN or LSTM} sub-networks (to capture morphological patterns, e.g., that many condition names end with "-itis") further improved recognition of rare or misspelled entities.

A key advantage of deep learning NER is the use of \textbf{word embeddings}. In medical NER, words like "aspirin" or "ibuprofen" will have vector representations that are close to each other if trained on clinical data, capturing the fact that both are medications. Early approaches used general embeddings (like word2vec or GloVe trained on Wikipedia) which helped, but soon \textbf{domain-specific embeddings} trained on medical corpora became available (e.g., embeddings from PubMed articles) and gave better results. These embeddings allow the model to leverage vast unlabeled text: even if "acetylsalicylic" (aspirin) was never in the NER training data, the model might still recognize it as a medication because its embedding is similar to known drugs.

By the late 2010s, \textbf{transformer-based models} took center stage. \textbf{BERT} (Bidirectional Encoder Representations from Transformers) introduced a powerful method of contextual representation, and researchers wasted no time applying it to biomedical NER. A common approach is to take a pretrained BERT (or a variant like BioBERT) and fine-tune it on a labeled medical NER dataset by adding a classifier for each token's label. This approach currently holds state-of-the-art performance on many NER benchmarks. For example, on the 2010 i2b2/VA clinical concept extraction challenge, fine-tuned transformers achieve substantially higher F1-scores than earlier RNN or CRF systems. Deep models are especially strong in handling \textbf{varied phrasing and contexts} – they can learn that "He was started on metformin" implies "metformin" is a Medication even if not explicitly capitalized or listed, and that "metformin" and "Glucophage" (brand name) refer to the same concept based on context usage.

Another trend in modern NER is using \textbf{multiple corpora and multi-task learning} to improve generalization. Recent systems like \textbf{HunFlair} combined training data from a wide range of biomedical NER datasets (covering genes, diseases, chemicals, etc.) to create a more robust model that doesn't overfit to one style of text \cite{Weber2023}. This addresses a common deep learning issue: a model trained only on, say, discharge summaries might falter on radiology reports; multi-corpus training alleviates this. There are also \emph{zero-shot} or \emph{few-shot} NER approaches emerging, where models like \textbf{GLiNER} (a zero-shot NER method) attempt to recognize entities without direct training on a specific dataset, by leveraging general language understanding or definitions of entities. While these are promising, they usually still trail well-trained supervised models in accuracy.

In summary, deep learning has become the dominant paradigm for medical NER due to its superior performance. The combination of powerful architectures (BiLSTM, CNN, Transformer), unsupervised pretraining (embeddings, language models), and larger annotated datasets has pushed concept extraction to new heights. Challenges remain – especially the need for large labeled datasets and ensuring the models handle edge cases correctly – but the gap between human annotator performance and automated NER has narrowed significantly in the clinical domain.

\subsection{Biomedical Language Models}

A major factor in the success of modern medical NLP is the creation of \textbf{biomedical language models}. These are large neural networks (typically transformer architectures) pretrained on huge amounts of biomedical text in an unsupervised manner, then fine-tuned for specific tasks. Notable examples include \textbf{BioBERT}, \textbf{ClinicalBERT}, \textbf{BlueBERT}, \textbf{SciBERT}, \textbf{PubMedBERT}, and more. The motivation is that while a general model like BERT learns English from Wikipedia and Books, it might not fully understand the specialized vocabulary and style of clinical narratives or biomedical literature. By further pretraining on domain text, the model picks up domain-specific terminology and context. For instance, BioBERT continued training BERT on 4.5 billion words from PubMed abstracts and PMC full-text articles. ClinicalBERT \cite{Alsentzer2019} trained on a corpus of EHR notes (MIMIC-III), exposing it to clinical shorthand and patient-related text. SciBERT was trained on scientific papers (biomedical and computer science) and even introduced a new vocabulary to better handle scientific terms.

These biomedical language models have unequivocally improved the performance of NLP tasks in healthcare. They serve as \textbf{foundational models} that can be fine-tuned with relatively little data. For example, a BioBERT-based NER model can outperform a vanilla BERT model on medical entity extraction by a large margin, because BioBERT "knows" words like "bradycardia" or "EKG" from its pretraining. In one comparative study, BioBERT was shown to \textbf{"largely outperform BERT and previous state-of-the-art models in a variety of biomedical text mining tasks."} This includes NER and relation extraction tasks on benchmarks such as the JNLPBA (bioentity recognition) or ChemProt (chemical-protein relation) dataset. Similarly, ClinicalBERT has been found to better capture context in clinical notes than general BERT, e.g., in extracting smoking status or identifying temporal events in a patient timeline. These models often come with domain-specific tokenizers – for instance, SciBERT's tokenizer includes more scientific terms – which further aids in representing domain phrases succinctly (treating "heart\_failure" as one token rather than two, for example).

Another development is \textbf{continual pretraining and task-specific pretraining}. For instance, a model like \textbf{BioMegatron} or \textbf{PubMedGPT} might be trained from scratch on biomedical text with even more parameters (bringing in the paradigm of GPT-style generative models to biomedical domain). There are models targeting clinical notes specifically (some being developed by companies or research groups that cannot be fully open due to patient data privacy). The general finding is that \textbf{domain knowledge in language models leads to better understanding}: a model that has read thousands of oncology papers will more likely flag "PD-1 inhibitor" as a Drug entity and know it's used in cancer treatment context, compared to a general model that might not recognize the term at all.

Biomedical language models do have limitations: they are usually large (requiring substantial computational resources), and they can still struggle with extremely rare or new terms (e.g., a newly approved drug that wasn't in the training data). They also inherit any biases or errors present in the biomedical literature. But overall, they represent the state-of-the-art starting point. In this thesis, we leverage such models for tasks like NER and potentially for relation extraction, expecting their domain-tuned knowledge to enhance performance. Furthermore, the comparison between a \textbf{medical-specific model} and a \textbf{general model} (the focus of our comparative analysis in Chapter 4) is essentially a comparison of the value of this domain pretraining – a topic of great interest in current research.

\section{Entity Linking Techniques}

\subsection{Knowledge Base Linking Methods}

Once named entities are recognized in text, the next step often needed is \textbf{entity linking} (also called entity normalization or grounding). This means mapping the text span (e.g., "aspirin") to a canonical identifier in a knowledge base (KB) (e.g., the concept code for Aspirin in a medical ontology). In the medical domain, entity linking is crucial for downstream applications because it resolves synonyms and variations to a standard reference (for example, linking "heart attack" and "myocardial infarction" to the same concept).

A variety of techniques have been developed for entity linking in biomedicine, ranging from simple string matching to complex machine learning models:

\begin{itemize}
\item \textbf{String Matching and Dictionary Lookup:} The most straightforward method is to use dictionaries of concept names from a knowledge base and attempt to match entity mentions to those names. \textbf{Traditional techniques for entity normalization performed recognition and linking in one step}, essentially scanning text for any substring that appears in the KB. Tools like \textbf{MetaMap} (for UMLS concepts) and \textbf{cTAKES dictionary lookup} fall in this category. They often allow some fuzziness (for example, ignoring case, minor typos, or word order) to match mentions to KB terms. Rules can be added to handle common variations (e.g., stripping plural 's' or matching acronyms). These methods are precision-oriented; they may miss entities that don't exactly match a known term. They also may struggle when a single mention has multiple possible mappings (ambiguity) without additional context.

\item \textbf{Rule-based and Heuristic Linking:} Some systems add heuristic rules to improve matching, essentially a step up from raw dictionary lookup. For instance, a rule might prefer a match to a concept whose semantic type fits the context (if "aspirin" is found in a Medication list section, prefer mapping it to a drug concept over a plant concept). Another heuristic approach is to use \textbf{co-occurrence}: if two candidate concepts often appear together in literature, that might influence selection. Traditional \textbf{rule-based normalization} systems like \textbf{GNormPlus} for gene names combined dictionary matching with custom post-processing rules to decide on the best gene ID. These approaches need domain expertise to create the rules and are often tailored to specific types of entities (genes, diseases, etc.).

\item \textbf{Machine Learning-based Linking:} Moving beyond hard rules, researchers introduced ML models for the linking step, especially as the number of candidate concepts can be very large (UMLS has millions of concepts). One approach is to formulate linking as a \textbf{ranking problem}: given a mention and a list of candidate concept IDs (those whose names or synonyms match closely), rank them to pick the best one. An early example is \textbf{DNorm} \cite{Leaman2013} which addressed disease name normalization by training a pairwise learning-to-rank model. It would take a mention and two candidate names at a time and learn which one is more likely correct. Features for such models include textual similarity scores, context vectors, concept popularity, etc. Another example, \textbf{TaggerOne} \cite{Leaman2016}, used a semi-Markov model to jointly recognize and normalize diseases and chemicals, learning parameters that consider both the context and the candidate concept's features. In these ML approaches, having training data (e.g., manually linked mentions in text) is critical. They often outperform pure dictionary lookups by learning to disambiguate based on usage context.

\item \textbf{Neural Embedding-based Linking:} With the advent of deep learning, a trend is to embed both mentions and knowledge base terms into a vector space and compute similarities. The idea is to learn an encoder (often a neural network) that converts any entity mention into a vector, and similarly each KB concept (perhaps by its name and definition) into a vector, such that the correct concept is closest to the mention in this space. Researchers have tried \textbf{character-level BiLSTMs} or CNNs to encode mentions and concept names. For example, Phan et al. \cite{Phan2019} train a char-BiLSTM that takes a mention string and a candidate concept name and learns to output high similarity if they match. They use multiple objectives, like one that maximizes mention-concept similarity and another that accounts for context coherence. Similarly, other work used CNNs on character n-grams of mentions with a loss function to differentiate the correct concept from incorrect ones.

\item \textbf{Pretrained Language Model for Linking:} The latest methods use transformers (BERT-based models) for entity linking. One prominent example is \textbf{BioSyn} \cite{Sung2020}, which uses BioBERT as an encoder for mentions and concept names. During training, BioSyn fine-tunes BioBERT such that the vector for a mention is close to vectors of its true concept's names and far from others. At inference, BioSyn can encode a mention and then do a fast nearest-neighbor search among all concept embeddings to find the best match. These methods achieve very high accuracy on biomedical linking benchmarks because the language model captures a rich understanding of synonyms and context. Some extensions incorporate hierarchical knowledge (e.g., if two concepts are parent-child in ontology, use that as a feature or constraint).
\end{itemize}

Each of these methods has trade-offs. Simpler methods (string match) are fast and don't require training data, but can't resolve ambiguity well. Advanced methods (neural linking) are more accurate in principle but require significant computation and annotated training data. Moreover, some neural methods struggle with the \textbf{scale} of biomedical KBs – performing a vector search over millions of concepts can be slow or memory-intensive, which is why approximate search or pruning strategies are used. In practice, many systems adopt a \textbf{hybrid approach}: e.g., use quick string matching to get candidates, then a learned model to pick the best, combining speed with accuracy. Our work specifically leverages \textbf{SciSpacy's} linking (described below) for candidate generation, which is a strong unsupervised method, and we incorporate confidence scoring and domain rules to refine the results.

\subsection{UMLS and MeSH Knowledge Bases}

In the biomedical domain, the two predominant knowledge bases used for entity linking in text are \textbf{UMLS} and \textbf{MeSH}, among others. It is important to understand their nature and why they are favored over general-purpose KBs (like Wikipedia or Wikidata) for clinical NLP.

The \textbf{Unified Medical Language System (UMLS)} is a comprehensive metathesaurus maintained by the U.S. National Library of Medicine. It aggregates over 150 biomedical vocabularies and ontologies into a unified framework. UMLS provides a concept unique identifier (CUI) for each concept, and links that concept to all its names (synonyms, abbreviations, variants in different source vocabularies) and relevant semantic types. For example, the drug aspirin has a CUI (C0004057) which ties together names like "Aspirin", "Acetylsalicylic Acid", brand names, etc., and identifies it as a pharmacologic substance (semantic type). UMLS is \textbf{massive}, containing over \textbf{3 million concepts} drawn from sources like SNOMED CT, RxNorm, ICD, MeSH, etc.. This breadth is one of its strengths – it covers clinical terminology (diseases, procedures), medications, devices, organisms, and even health administration terms. When linking entities, mapping to UMLS CUIs allows integration with electronic health records and clinical decision support systems, which often use these standard codes. It also facilitates aggregation: one can link "heart attack" in text to a UMLS CUI and then know that it corresponds to ICD-10 code I21 (for myocardial infarction), etc.

\textbf{MeSH (Medical Subject Headings)} is another widely used vocabulary, primarily for indexing journal articles (PubMed). MeSH is a curated hierarchical taxonomy of biomedical terms, with about \textbf{30,000 main headings}. These include diseases, chemicals, anatomical terms, etc., organized into a tree (so "Heart Diseases" might be a category with children like "Myocardial Infarction", etc.). MeSH terms are used by librarians to tag articles for easier retrieval. In NLP, MeSH can be used as a linking target, especially for biomedical literature text (e.g., linking an article's key terms to MeSH can help find related articles). SciSpacy, for instance, supports linking to MeSH as an alternative to UMLS. MeSH is \textbf{smaller and more curated}; each concept is well-defined and has synonyms listed, but it won't include every clinical term (for example, very specific procedure names or abbreviations used in clinical notes may not be MeSH terms).

Using medical-specific KBs like UMLS/MeSH has clear advantages over general-purpose knowledge bases for this task. First, \textbf{coverage}: general KBs (like Wikipedia) might not list every laboratory test name or obscure medical acronym, whereas UMLS likely does (through one of its source vocabularies). Second, \textbf{terminology consistency}: UMLS provides a unified identifier for concepts that appear under different names in different contexts. This is crucial in clinical data where synonyms and acronyms abound. Third, \textbf{domain relevance}: UMLS/MeSH have semantic types and relationships tailored to medicine (e.g., "Drug A treats Disease B" relationships exist in UMLS through RxNorm or SNOMED CT relations), enabling richer graph construction later. In contrast, a general KB might have the drug and disease as unrelated entries or not capture the treatment relation explicitly.

However, UMLS is \emph{so} comprehensive that it introduces ambiguity; many concepts and names overlap. This makes linking challenging – hence the need for algorithms as described. Additionally, UMLS requires a license (free for research but with a process), which is another reason why specialized tools exist to handle it (they often come with the UMLS data pre-packaged for convenience). In our project, we predominantly use \textbf{UMLS} as the linking target, given its broad coverage, and occasionally reference \textbf{MeSH} when discussing alternate approaches or quality of concepts.

\subsection{SciSpacy's TF-IDF Character N-gram Matching}

\textbf{SciSpacy} \cite{Neumann2019} is an open-source library developed by the Allen Institute for AI that extends the popular spaCy NLP framework to scientific and biomedical text. It includes pre-trained models for biomedical NER and, importantly, an \textbf{EntityLinker} component that can link text spans to knowledge base entries. Our system leverages SciSpacy for entity linking, so we detail its approach here.

SciSpacy's EntityLinker uses a \textbf{sparse vector model based on TF-IDF over character n-grams} to perform linking. The process can be summarized as follows: SciSpacy comes with a pre-built knowledge base index (for example, an index of UMLS concept names). Each concept name (and its synonyms/aliases) in the KB is indexed by the character 3-grams it contains. For instance, the term "aspirin" would be broken into character trigrams: "asp", "spi", "pir", "iri", "rin". This is done for all terms, yielding a high-dimensional space of n-gram features. When an entity mention is extracted from text (say, "Aspirin 325mg"), SciSpacy takes the mention text ("Aspirin") and computes its TF-IDF weighted character n-gram vector. It then performs an approximate nearest neighbors search in the index of KB terms to find the closest matches by cosine similarity. In essence, it's looking for the concept whose name shares the most overlapping n-grams with the mention, with weighting that downranks very common n-grams. This method is a robust string matching strategy: it can handle minor spelling differences (because many 3-letter sequences will still match) and partial matches. It's similar to other string matching tools like \textbf{QuickUMLS}, which uses character n-gram overlap and heuristics.

The SciSpacy linker returns, for each entity span, a list of candidate KB identifiers along with a \textbf{similarity score} (confidence). By default, it may return up to a certain number (e.g., top 5) candidates for each mention, sorted by the TF-IDF similarity score. Each candidate has the KB concept ID (like a UMLS CUI) and the similarity score. For example, if the mention is "COPD", SciSpacy might return: CUI for "Chronic Obstructive Pulmonary Disease" (score 0.95), CUI for "Compulsive Obsessive Personality Disorder" (score 0.60) – illustrating a correct and a less likely candidate. It's then up to the user/system to decide how to use these; one might take only the top hit if the score is above a threshold, etc. SciSpacy provides the knowledge base object so one can also retrieve the concept name, definition, etc., for each CUI.

One valuable feature of SciSpacy is its integration of \textbf{abbreviation detection}. Clinical text is rife with abbreviations (e.g., "HTN" for hypertension, "T2DM" for type 2 diabetes). SciSpacy includes an \textbf{AbbreviationDetector} component based on the Schwartz \& Hearst algorithm. This component scans the document for patterns like "Full Form (ABBR)" and learns that the abbreviation "ABBR" stands for "Full Form" in that document. It also can infer abbreviations without explicit definitions by statistical means (though defined ones are easier). When the EntityLinker is configured with \texttt{resolve\_abbreviations=True}, it will attempt to link the \emph{long form} of an abbreviation rather than the short form. For example, if a note says "Patient has Chronic Obstructive Pulmonary Disease (COPD). COPD is worsening." – the AbbreviationDetector will note "COPD" = "Chronic Obstructive Pulmonary Disease". Then when linking, SciSpacy will treat the second "COPD" mention as if it were "Chronic Obstructive Pulmonary Disease", ensuring it links to the correct concept rather than trying to match "COPD" literally (which it could also do, but the long form provides more clarity). This greatly improves linking accuracy for abbreviations and acronyms.

The simplicity of SciSpacy's TF-IDF n-gram approach is actually a strength in practice: it's \emph{fast}, and it doesn't require any training data or machine learning to use. It can be applied to very large KBs (the heavy lifting is done by efficient nearest-neighbor search libraries). However, it does have limitations – it purely matches on surface forms and does not use context beyond the mention string itself. For example, "cold" could link to a disease (common cold) or a symptom (feeling cold) or a medication (Cold tablets), and SciSpacy will just return whichever term name is most similar, perhaps many candidates if "cold" appears in many concept names, without knowing which is meant in the text. That's where we might incorporate \textbf{confidence thresholds or post-filters}. SciSpacy allows filtering out candidates that lack a definition or are very low score, etc., which can be tuned. We leverage these features in our pipeline: we use SciSpacy to get candidate UMLS CUIs for each entity, then apply a confidence score cutoff (and some custom logic like preferring certain semantic types) to finalize the linking. Overall, SciSpacy's TF-IDF n-gram method provides a solid baseline for entity linking, capturing a large majority of cases especially when the entity mention closely resembles the canonical name or a known synonym. Harder cases (abbreviations, ambiguous short terms) are where additional strategies are needed, but SciSpacy gives us a strong starting point without needing to train a complex model from scratch.

\subsection{Challenges in Medical Entity Linking}

Linking clinical entities to a knowledge base is a challenging task due to several inherent issues in medical text and the nature of biomedical knowledge bases:

\begin{itemize}
\item \textbf{Synonymy and Term Variation:} Medical concepts often have numerous ways to be expressed. A single disorder might have a long formal name, several shorthand names, and colloquial descriptions. For instance, "heart attack", "myocardial infarction", "AMI", and "cardiac infarct" all refer to the same concept. A linking system must recognize these variants and unify them. UMLS helps by listing synonyms, but the text might contain something not in the synonym list (e.g., a misspelling or a very specific description). Ensuring that different surface forms map to one concept (and not accidentally to different but related concepts) is tricky. This is essentially a \textbf{recall} challenge – failing to link an entity because the exact wording wasn't seen before.

\item \textbf{Ambiguity (Polysemy):} Many medical abbreviations and terms are ambiguous. "RA" could mean rheumatoid arthritis, right atrium, renal artery, etc., depending on context. Even non-abbreviations can be ambiguous ("discharge" could be a verb or a noun (fluid), "mass" could be a tumor or an amount). When a mention can map to multiple KB entries, the linker has to pick the correct one. This is a \textbf{precision} challenge – linking to the wrong concept is often worse than not linking at all. Disambiguation requires using the context in which the term appears (neighboring words, document metadata, etc.), which simple string-based methods do not fully exploit. For example, the mention "COPD" in a respiratory clinic note clearly should link to lung disease, whereas in a psychology note "COPD" is unlikely to appear at all. Context-aware linking (as some neural methods attempt) is an ongoing challenge, especially with limited training data for those methods.

\item \textbf{Data Scarcity for Training:} Unlike NER where one can sometimes obtain labeled data by annotation, linking requires \textbf{mapping to concepts}, which is a more complex annotation task. Creating a gold-standard corpus where every entity mention is mapped to a concept like a CUI is time-consuming and requires domain experts. There have been some shared tasks (e.g., the 2019 n2c2 challenge had a concept normalization track for disorders) producing datasets, but these are relatively small and specific. The lack of large, general training corpora for entity linking means supervised machine learning approaches can overfit or fail to generalize. This is one reason why dictionary and heuristic approaches are still popular – they don't need training data. Research into distant supervision (using dictionary matches as pseudo-training data) or leveraging unlabeled data is ongoing to mitigate this.

\item \textbf{Knowledge Base Coverage and Granularity:} UMLS is huge, but not perfect. There might be concepts in the text that are \textbf{not present} in the KB or not at the right granularity. For example, UMLS might have a concept for "Type 2 diabetes" and "Type 1 diabetes", but what if a note says "brittle diabetes" (an old term for unstable Type 1 diabetes)? There may or may not be a direct concept for that. Choosing a concept that is too general or too specific is a challenge – should "brittle diabetes" map to Type 1 diabetes concept, or just diabetes mellitus unspecified? Such decisions can require nuanced understanding. Moreover, UMLS merges many sources, which can lead to duplicative or very similar concepts; linking systems might get confused between two codes that both essentially mean the same thing. Deciding which one is "correct" may be arbitrary (this is where application context matters – if you plan to later use the links for a particular purpose, you might prefer one vocabulary's concept over another).

\item \textbf{Abbreviations and Shorthand:} As noted, abbreviations are extremely common. While SciSpacy and similar can detect many, it's not guaranteed. Some abbreviations are ambiguous (context needed), and some are rare or user-specific. For example, a doctor might write "pt c/o CP" which means "patient complains of chest pain." Linking "CP" to "Chest Pain" requires not just knowing the expansion but also that chest pain is a symptom concept in the KB. Handling abbreviations often needs a multi-step approach: detect and expand the abbreviation, then link the expansion. If abbreviation expansion fails, linking the short form directly is low-confidence because it might map to dozens of concepts (CP could map to Cerebral Palsy, for instance, aside from Chest Pain). This remains a pain point in clinical entity linking.

\item \textbf{Context and Relation Constraints:} Often, whether a mention should be linked or what it links to depends on context or implicit relations. E.g., in "family history of breast cancer", the mention "breast cancer" should perhaps link to the concept of breast cancer \emph{but flagged as family history} (some systems treat this differently). Or in "denies chest pain", the concept is chest pain but the context is negated. While linking typically ignores negation (linking "chest pain" to the concept regardless), certain applications might want to encode that the patient \emph{does not have} that problem. Some recent research tries joint entity linking and relation extraction (like identifying negated concepts), but generally linking algorithms by themselves don't do this. It's a challenge left to either pre-processing (not marking negated concepts at all) or post-processing the graph.

\item \textbf{Scalability and Performance:} As mentioned, a high-end method like using BERT to encode every mention and compare with every concept would be extremely slow with millions of concepts. This scalability issue means that sometimes a simpler method is chosen as a compromise. Even with approximate nearest neighbor methods, memory usage is high for storing embeddings of all concepts. So there's a practical challenge of making entity linking both \emph{fast and accurate}. This is an active engineering problem – for our pipeline, we needed to ensure that linking each note does not become a bottleneck, which is one reason we opted for SciSpacy's efficient approach and then deal with a smaller set of candidates.
\end{itemize}

Due to these challenges, \textbf{no single linking method is perfect}, and mistakes can propagate to later stages (for example, linking "Cold" to the concept of "Common Cold" when the text meant "cold sensation" could lead a knowledge graph to erroneously indicate the patient had a cold infection). Our thesis addresses some of these by using confidence scoring (not accepting low-score links), abbreviation handling, and by constraining the types of entities we link (focusing on ones most useful for the graph). It's also worth noting that because of the above difficulties, many state-of-the-art tools still include a \textbf{manual or rule-based component} for linking certain entities or defaulting to dictionary matches when the fancy model is unsure. This acknowledgement in the community underscores that a combined approach (where simple methods handle easy cases and complex methods handle hard cases) is often the best practical solution – a philosophy we carry into our multi-model system.

\section{From Relationship Extraction to Knowledge Graphs}

\subsection{Relationship Extraction Methods}

Identifying relationships between medical entities is the step that transforms isolated pieces of information into structured knowledge. In text, a \textbf{relationship extraction (RE)} system might detect, for example, that a certain drug is indicated for a disease, or that one clinical finding is a symptom of a condition mentioned elsewhere in the text. Extracting such relations from unstructured text allows us to then build edges in a knowledge graph (connecting the nodes that were identified via NER and linking).

\subsubsection{Rule-Based Approaches}

Early approaches to relation extraction in clinical text were predominantly \textbf{rule-based or pattern-based}. These systems rely on human experts to define patterns that indicate a relationship. For instance, a simple rule might be: \emph{if a medication name and a condition appear in the same sentence with the word "for" or "to treat" in between, infer a Treats relation}. An example rule could target a template like "<Drug> for <Condition>" (e.g., "Prescribed \textbf{metformin} for \textbf{diabetes}"). Other patterns might use punctuation or specific trigger verbs ("caused", "due to", "associated with") to link an entity pair. Syntactic patterns can also be used: using a dependency parse of the sentence, one might write a rule that if an entity of type Disease is the direct object of "diagnosed with", link it as a Diagnosis relation for the patient.

Rule-based RE has the advantage of being \textbf{precision-oriented and interpretable}. If a relation is extracted, one can trace exactly which rule fired. In critical applications (like identifying adverse events), a carefully crafted rule might be preferred for its reliability. Furthermore, rules can incorporate domain knowledge easily; for example, you might add a rule that "if X is a Finding and Y is a Disease and they are connected by 'of', then X is a symptom of Y". This leverages knowledge of grammatical constructs common in clinical writing.

However, as with rule-based NER, the \textbf{coverage is a major issue}. Language is very variable, and it's impossible to enumerate all ways a relationship might be expressed. Co-occurrence in the same sentence might signal a relation, but not always (it could be coincidental). Conversely, a relation might be implied across sentences (which simple intra-sentence rules would miss). Pattern-based methods also struggle with \textbf{negation and temporal expressions} that modulate relationships. For instance, the sentence "No evidence of metastasis to the lung" contains "metastasis" and "lung" which might match a pattern "X metastasis to Y" normally indicating a spreads-to relation, but here it's negated. Handling such nuances requires either very complex rules or a hybrid with some semantic understanding. Indeed, a known drawback is \textbf{low recall}: pattern-based methods often only capture a subset of true relations, as noted by recent studies \cite{Laue2024}. They tend to miss relations phrased in unexpected ways and can be brittle if the text deviates even slightly from the anticipated structure.

Despite that, rule-based approaches are still used in certain settings. For example, a hospital might implement a rule-based system to flag specific events (like "allergy to X" relations from notes) because it's easier to maintain and verify. Also, rule-based extraction can complement statistical methods – some systems do a first pass with rules to catch obvious cases with high precision, and then handle the rest with machine learning. In summary, rule-based RE methods laid important groundwork and are characterized by high precision and transparency, but require extensive knowledge engineering and don't scale well to the diversity of real-world text.

\subsubsection{Machine Learning Techniques}

As annotated datasets of clinical text became available (e.g., through shared tasks like i2b2 challenges), \textbf{machine learning (ML)} approaches to relation extraction gained prominence. In these approaches, rather than manually specifying what linguistic patterns indicate a relation, one trains a model on examples of text where entities are labeled and the relation between them (or lack thereof) is annotated. The model then learns to predict relations in new text.

\textbf{Feature-based ML:} Early ML methods for clinical RE often used classifiers such as Support Vector Machines (SVM) or Logistic Regression. These models require the text to be converted into a feature vector. Researchers engineered features capturing various signals: the words between the two entities (for example, presence of words like "for", "due to" might signal certain relations), the order of entities (does treatment precede disease or vice versa), entity types (Drug-Disease pair vs. Disease-Disease pair), syntactic parse features (like the shortest path in the dependency tree between the two entities), and section information (mention in the "Medication" section of a note vs. "Family History" section changes interpretation). A concrete example: to determine if a Medication and a Problem are in an "adverse reaction" relation, features might include: whether the problem is immediately after the med ("Penicillin allergy"), whether the verb "causes" or "induced" connects them, etc. Using such features, an SVM could be trained to classify yes/no for a specific relation type. Some systems trained one classifier per relation type, others a single multi-class classifier to decide which relation (including "no relation"). Feature-based ML was quite successful in competitions; many top entries in the 2010 and 2012 i2b2 challenge for relation extraction were SVMs with cleverly designed features. They substantially beat pattern-matching in recall, while maintaining decent precision. For instance, an SVM could learn a generalization like "if entity1 is a drug and entity2 is a disease and 'for' is in between, it's likely a treatment relation" without someone explicitly coding that rule, by observing many training examples.

One limitation, however, is that these models \textbf{don't generalize well to contexts not represented in features}. If an important cue isn't captured as a feature, the model can't learn from it. Plus, they tended to require a lot of domain-specific tweaking – essentially moving the burden from writing rules to writing good features.

\textbf{Neural Network ML:} As with NER, relation extraction also moved towards neural approaches around mid-2010s. Instead of manual feature extraction, models like \textbf{Convolutional Neural Networks (CNN)} or \textbf{Recurrent Neural Networks (RNNs)} were applied to the sequence of words between and around the target entities. A classic architecture is: represent the sentence (with marked entity positions) as an embedding sequence, feed into a CNN or LSTM, and then have the network output a relation class. The network's internal weights will learn to emphasize certain patterns. For example, a CNN with filters might learn that "X \emph{was started on} Y" is a strong indicator of a medication X being used for condition Y, without us explicitly coding that phrase. RNNs (like LSTMs) can capture longer dependencies and have been used to detect relations spanning clauses or sentences. More recently, transformer models (like BERT again) have been fine-tuned for relation classification: one common method is to insert special tokens around the two entities in the text and then feed it to BERT and use the output to classify the relation. This has achieved state-of-the-art results on many benchmarks. The fine-tuned BERT implicitly leverages both local context and global semantics to decide if, say, a "Treatment" relation exists between a drug and a condition in a sentence.

The ML approaches (especially neural ones) greatly improved \textbf{recall and adaptability}. They can find relations expressed in subtle ways that rules might miss, and they don't require human experts to enumerate patterns. For example, a neural model might catch that in "She responded well to metformin, her diabetes is now controlled," there is an implicit treatment relationship, even though it's not in a simple pattern. However, ML models, particularly deep ones, need substantial labeled data to train effectively. In clinical domains, getting thousands of examples of each relation type annotated is challenging. As a result, many neural models are trained on combined datasets or even on \emph{synthetically generated} data to supplement. There's also the issue of \textbf{error interpretability}: when a learned model makes a mistake (e.g., linking a symptom to the wrong condition), it can be hard to understand why, making debugging and refining difficult.

That said, the current trend is clearly towards neural RE when data is available, as it consistently outperforms feature-based methods. Some hybrid systems feed outputs of neural models into rule-based frameworks or vice versa to get the best of both worlds. For instance, one might use a quick classifier to filter obvious non-relations and then apply precise rules on the remainder, or use rule outputs as additional features for an ML model. In our project, for the \textbf{relationship extraction component}, we explore using large language models (see next section) – essentially taking the neural approach to the extreme by leveraging models trained on enormous general text corpora.

\subsubsection{Large Language Models for Relationships}

The latest frontier in relationship extraction is the use of \textbf{Large Language Models (LLMs)}, such as GPT-3, GPT-4, PaLM, Bloom, etc. These models are trained on diverse internet-scale text and exhibit an ability to perform tasks via prompting – even tasks they weren't explicitly trained for – by virtue of their general language understanding and world knowledge. In the context of biomedical relation extraction, researchers have started investigating how well LLMs can identify and extract relationships without explicit task-specific training. The findings so far are promising: \textbf{"Large Language Models have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios."} \cite{Laskar2025}.

There are a couple of ways LLMs are used for RE:

\begin{itemize}
\item \textbf{Prompt-based Classification:} You can present the LLM with a prompt that describes the relation extraction task and the specific instance. For example, to classify the relation between a gene and a disease in a sentence, one might prompt GPT-4 with: \emph{"Task: Determine if there is a positive, negative, or no relation between the gene [BRCA1] and the disease [breast cancer] in the following sentence: 'The BRCA1 mutation is associated with a higher risk of breast cancer.' Answer with '1' for positive relation and '0' for no relation."}. The model then outputs the label. This approach effectively turns the RE task into a natural question-answer format. Researchers have found that with carefully engineered prompts (and possibly a few examples provided in the prompt for few-shot learning), LLMs can achieve accuracy comparable to traditional supervised models on some benchmarks. The advantage is that this requires \textbf{no training phase} on task-specific data – the model is leveraged as-is. This is extremely useful in clinical RE because, as noted, large annotated datasets are scarce. An LLM like GPT might already "know" typical relations (e.g., that certain drugs treat certain diseases) and pick up on textual cues to apply that knowledge.

\item \textbf{Prompt-based Extraction (Generative):} Instead of classifying from given options, we can ask the LLM to directly extract relations in a more open-ended way. For instance: \emph{"Extract all <drug, condition> treatment pairs from the text: …"}. The model could generate a list of pairs it believes are in a treatment relationship. Another example: given a clinical note, ask the model to output structured data (perhaps as JSON) listing each problem and any associated tests or medications mentioned. This uses the generative nature of LLMs to do information extraction in a flexible way. Tools like GPT-4, when properly prompted, can sometimes parse complex text and output triples or relationships with surprising accuracy. They inherently do a form of joint understanding of NER and RE – because they identify entities and the relation simultaneously when generating the output.
\end{itemize}

Using LLMs for RE also brings some \textbf{new challenges}. Firstly, evaluation is tricky: LLMs might express the relationship in words different from the gold standard, which makes automatic evaluation hard (hence research on using LLMs themselves as evaluators). Secondly, they can \textbf{hallucinate} – produce a relation that sounds plausible but isn't actually stated in the text. For example, given a patient case, an LLM might "fill in" a likely treatment that the text didn't explicitly mention. This is dangerous in a factual extraction context. Ensuring the model sticks to the text evidence is an area of active research (techniques like chain-of-thought prompting or adding instructions to not include unmentioned facts). Thirdly, \textbf{privacy and deployment}: Many powerful LLMs are accessible only via cloud APIs, which is problematic for patient data. This has spurred interest in smaller domain-specific LLMs that could be run locally on hospital data (the "MedGPT" sort of efforts).

Despite these issues, the ability of LLMs to perform zero-shot or few-shot RE is a potential game-changer. It can dramatically reduce development time (no need to train a model for each new relation type; just prompt it appropriately) and can adapt to multiple tasks easily. Our thesis specifically compares a \textbf{medical-specific model} (MedGemma3-4b) with a \textbf{general-purpose model} (Gemma3-4b) for relation extraction, which aligns with examining this trend: How much does a general LLM know about clinical relationships out-of-the-box, versus a model tuned with medical knowledge? Prior work indicates general models can be strong but domain-tuned models still have an edge in specialized accuracy. For instance, GPT-3 might do well, but a smaller model fine-tuned on biomedical text might catch nuances GPT-3 misses. We will explore this in Chapter 4. In summary, LLMs represent the cutting-edge approach to extracting relationships, offering flexibility and leveraging massive implicit knowledge, and are likely to play an increasing role in clinical information extraction systems.

\subsection{Knowledge Graph Construction}

\subsubsection{Graph Database Technologies}

A \textbf{knowledge graph} is essentially a network of entities (nodes) linked by relationships (edges) that represent facts or assertions. Once we extract entities and relationships from text, we need to store them in a structured way for querying and analysis. This is where \textbf{graph database technologies} come into play. Unlike traditional relational databases that use tables, graph databases are designed to store and query data in terms of nodes and edges, which is a natural fit for knowledge graphs.

There are two main paradigms for graph data management: \textbf{RDF triple stores} and \textbf{property graph databases}.

\begin{itemize}
\item \textbf{RDF Triple Stores:} These databases store data as triples (subject, predicate, object), in line with the Resource Description Framework (RDF) standard from the semantic web community. For example, (Aspirin, treats, Diabetes) would be a triple. Triple stores (like Apache Jena TDB, Virtuoso, GraphDB) often use the SPARQL query language to retrieve data. SPARQL queries look somewhat like SQL but are tailored to graph patterns – you specify a triple pattern with variables and the engine finds matches in the data. RDF systems excel at enforcing a schema/ontology (you can have formal ontologies in OWL that the data adheres to) and at integrating data from multiple sources via shared identifiers. They're popular in academic knowledge graph work and where semantic interoperability is key (like linking to external ontologies).

\item \textbf{Property Graph Databases:} These are more commonly used in industry and practical applications. In a property graph model, each \textbf{node} can have labels and properties (key-value pairs), and each \textbf{relationship (edge)} has a type and can also have properties. For example, we might have a node of type "Medication" with properties like \texttt{name="Aspirin"} and \texttt{dosage="325mg"} (if storing patient-specific data), and a node of type "Condition" with \texttt{name="Diabetes Mellitus"} and perhaps a severity property. An edge of type "TREATS" from the Aspirin node to the Diabetes node could have a property \texttt{source="doctor\_notes"} indicating how we know this (from text). This property graph model is very flexible – we can attach various contextual data directly on nodes/edges. \textbf{Neo4j} is a leading database in this category and is known for being user-friendly and highly optimized for graph queries. Other examples include Amazon Neptune (which actually supports both RDF and property modes), JanusGraph, and TigerGraph.
\end{itemize}

Graph databases allow efficient traversal of the graph. For instance, a query like "find all medications that treat diseases which have symptom X" would involve traversing from a symptom node X, to disease nodes connected by "hasSymptom" edges, then from those diseases out via "treatedBy" or "treated\_with" edges to medication nodes. Doing this with SQL would be complex and slow if the data is heavily relational and many joins are needed, whereas graph DBs are optimized for such multi-hop queries.

Moreover, graph databases align with how we conceptualize knowledge: they can directly store an edge saying \textbf{Drug->treats->Disease}, which is more intuitive than having to indirectly connect them via foreign keys in tables. In a knowledge graph, relationships are first-class citizens – you can query them, filter on them (e.g., find all "inhibits" relations between genes and proteins, etc.), and even put constraints on paths.

For our project, the property graph model is suitable because we are building a graph of \emph{specific extracted facts} (not just a generic ontology). Each extracted entity becomes a node, with properties like its type (Problem, Test, Treatment, etc.) and maybe the UMLS CUI as an attribute. Each extracted relation becomes an edge, with a type (like "ASSOCIATED\_WITH" for symptom-disease, "TREATS" for drug-disease, etc.) and possibly a confidence score property or provenance (which note it came from). This structured representation then lives in a graph database so we can query it to answer questions or analyze the network.

In summary, graph database technologies provide the backbone for \textbf{storing} and \textbf{querying} the knowledge graph that results from our NLP pipeline. They handle the heavy lifting of managing connections at scale. The choice between RDF vs property graph often comes down to use case; we have chosen a property graph approach (Neo4j) for its straightforwardness and the ability to easily attach attributes to nodes/edges which is useful for tracking metadata of extracted info.

\subsubsection{Neo4j and Cypher Query Language}

\textbf{Neo4j} is a widely-used graph database that implements the property graph model and has become a standard for many knowledge graph projects. It's known for its performance, developer-friendly interface, and a rich ecosystem of tools. In the context of our thesis, Neo4j serves as the platform where the extracted knowledge graph is built and stored. Here's why we chose Neo4j and how we use it:

\begin{itemize}
\item \textbf{Data Model:} In Neo4j, data is stored as nodes and relationships. We design a schema (or \emph{graph model}) where, for example, we might have labels like \texttt{:Person}, \texttt{:Condition}, \texttt{:Drug}, etc., for nodes, and relationship types like \texttt{DIAGNOSED\_WITH}, \texttt{TREATS}, \texttt{CAUSES}, etc., for edges. Each node can have one or more labels and a set of properties; each relationship has a type and can also carry properties. This is very convenient to represent complex clinical data – e.g., a patient node can be linked to multiple condition nodes via \texttt{HAS\_CONDITION} edges, and each such edge could have a property \texttt{onset\_date} or \texttt{status} (active/resolved). In our extracted graph (which is de-identified and aggregated, not focused on individual patients), we likely have nodes representing medical concepts (like diseases, symptoms, medications) and edges representing the relations found in text (like "<Symptom> ASSOCIATED\_WITH <Disease>"). Neo4j's ability to handle property graphs means we can store things like confidence scores or context (like which model extracted this relation) as properties on the edge, which is very useful for later analysis or filtering.

\item \textbf{Cypher Query Language:} Neo4j uses \textbf{Cypher}, a declarative query language specifically designed for querying graphs. Cypher's syntax is often described as "SQL for graphs" but with pattern matching. For instance, a Cypher query to find what diseases a drug treats might look like:

\begin{verbatim}
MATCH (d:Drug {name:"Aspirin"})-[:TREATS]->(c:Disease)
RETURN c.name;
\end{verbatim}

This would find all \texttt{:Disease} nodes connected via a \texttt{TREATS} relationship from the \texttt{:Drug} node named "Aspirin", and return the disease names. Cypher uses ASCII-art style notation, where parentheses \texttt{()} denote nodes and \texttt{-[]->} denotes directed relationships. One can put conditions inside, like \texttt{\{property:value\}} or add variables. It's quite expressive – you can do aggregations, ordering, etc., similar to SQL, but the power is in pattern matching across arbitrary path lengths. For example, finding if there's any connection between two concepts within 3 hops is a simple Cypher query with path patterns. In our project, we generate Cypher queries programmatically to insert the data. For each extracted entity, we may do a \texttt{MERGE (n:Condition \{cui:"C0012345", name:"Diabetes"\})} to create or find a node, and for each relation, a \texttt{MERGE} for the edge like

\begin{verbatim}
MERGE (d:Drug \{cui:"C0004057"\})-[:TREATS \{source:"note1"\}]->
(c:Condition \{cui:"C0011849"\})
\end{verbatim}

Using \texttt{MERGE} ensures we don't create duplicates if the node already exists. We also may use Cypher for querying the graph once built, to answer questions like "how many unique medications were linked to each condition," or "find an example subgraph of a patient's note information." Cypher's readability (almost like describing the pattern you want) makes it easier for us to verify that the graph is constructed correctly.

\item \textbf{Advantages of Neo4j:} Apart from the model and query language, Neo4j is optimized for \textbf{graph traversals}. Queries that chase pointers through data (which in SQL would be multiple JOINs) are very fast in Neo4j even as the graph grows, because of index-free adjacency (each node directly knows its connected nodes). Neo4j also has features like full-text indexing (if we want to index node names for quick lookup), and APOC (a library of procedures) which can do things like computing shortest paths or other graph algorithms. There's also a web-based browser interface that lets you visualize query results, which is great for exploring the knowledge graph manually – you can literally see nodes and relationships plotted, which can reveal patterns or errors (like if an entity wasn't linked properly, it might appear isolated).

\item \textbf{Cypher Query Generation:} A part of our methodology (Chapter 3) is dedicated to how we translate extracted information into Cypher queries (Section 3.6.1). In practice, for each clinical note processed, our pipeline will output a series of Cypher commands to build that portion of the graph. We ensure queries are batched and transactions are used to make insertion efficient. The result is a Neo4j database containing the consolidated knowledge graph from all notes.
\end{itemize}

In summary, \textbf{Neo4j} provides a robust platform for our knowledge graph, and \textbf{Cypher} allows us to interact with that graph in a flexible way – whether to populate it or to retrieve insights. Many published clinical knowledge graph projects (and industry solutions) use Neo4j due to these strengths \cite{Zimbres2024}. By using Neo4j, we align with best practices and ensure that our graph is not just a conceptual outcome, but a queryable database that can answer complex questions about the data model we've constructed.

\subsection{Clinical Knowledge Graph Applications}

Knowledge graphs (KGs) in the clinical domain unlock a range of powerful applications by enabling connections across different types of medical data and knowledge. By converting unstructured text into a graph of entities and relationships, we create a structure that can be traversed and analyzed to support both clinical care and research. Here we review some key applications of clinical knowledge graphs, as reported in literature and practice \cite{Milvus2025}:

\begin{itemize}
\item \textbf{Integrating Patient Data for Holistic View:} One immediate use of KGs is to break down data silos in healthcare. Patient information is often fragmented across EHR free-text notes, structured tables, lab systems, etc. A knowledge graph can serve as a unifying layer by representing a patient and all related entities (conditions, drugs, lab results, procedures, family history, etc.) as a connected subgraph. For example, a KG might have a node for a patient that links to nodes for each diagnosis they have, each medication they are taking, and key findings from their last visit. Even without identifying the patient (for privacy, one could use an anonymous patient ID), this structured representation allows complex queries. A clinician or system could query: "Find all patients who have Diabetes and are on Drug X and have lab result Y > threshold," which is essentially a graph pattern search. Or in a single patient context: "Show me all known allergies and medications for this patient and any documented interactions." The graph naturally brings these pieces together. A published example of this concept is the use of KGs for a \textbf{Patient Timeline} or summary: by linking problems to treatments and outcomes over time, one can generate a chronological story of care. This integration is especially important for chronic diseases with long histories and multiple care providers. In a graph, if a patient's node is connected to a concept node "Type 2 Diabetes", and that concept node might connect further to a "Diabetes Mellitus" guideline node (as knowledge), one could quickly see if the patient's care aligns with recommended care (because the guideline node might connect to a recommended medication node, etc.). In our work, while we do not maintain patient-specific graphs (due to working on de-identified aggregated text), the principles learned can apply to patient-centric KGs in operational settings.

\item \textbf{Clinical Decision Support (CDS):} Knowledge graphs can enhance decision support systems by encoding medical knowledge and enabling reasoning over it. For instance, consider a KG that includes not just patient data but also nodes and edges representing clinical guidelines or best practices. One could model a rule like "Patients with Condition X should receive Test Y every 6 months" as a relationship in the graph between the Condition node and a Procedure node (perhaps with an edge type like "REQUIRES\_TEST"). Then, by linking patient nodes to their condition nodes and seeing if the "REQUIRES\_TEST" edge is followed by a "HAS\_DONE\_TEST" edge (for that patient), you can query for patients who are missing a recommended test. Another scenario is a graph that connects drug nodes to condition nodes they treat and contraindication nodes they conflict with. A CDS system could traverse the graph: given a patient's conditions and current medications (all nodes connected to that patient), are there any contraindication edges among those? If yes, alert the clinician. Some research prototypes have used knowledge graphs to do things like suggest diagnoses: they take the patient's symptoms and existing conditions as a subgraph, then look for a disease node that is connected to many of those findings (using the graph of medical knowledge that symptoms link to diseases) – essentially performing a reasoning that a doctor might do mentally. Graph queries (via Cypher or SPARQL) can implement such logic in a straightforward way by pattern matching. For example, find a disease node that has edges to all of a set of symptom nodes that the patient has, rank by how many of the patient's findings connect, etc. This could be used to generate differential diagnoses or catch something that might have been overlooked. In sum, KGs allow \textbf{automated inference} (or at least advanced querying) that goes beyond looking at each piece of data in isolation, thereby supporting clinical decisions with a more holistic consideration of relationships.
\end{itemize}

\section{Related Work Summary and Research Gap}

In this chapter, we reviewed the literature across the spectrum of our task: from natural language processing in the clinical domain (entity recognition and linking) to relationship extraction and knowledge graph construction. The existing body of work demonstrates substantial progress in each of these individual areas:

\begin{itemize}
\item \textbf{Clinical NLP Maturity:} The evolution from rule-based systems to statistical ML, and now to deep learning and large language models, has greatly enhanced the accuracy of extracting information from clinical text. There are robust tools and models for medical NER (e.g., BiLSTM-CRF models, BioBERT-based taggers) that can identify a wide range of entity types with high performance. Similarly, relation extraction techniques have evolved to capture increasingly complex relationships, especially with transformers and prompt-based LLM methods leading recent benchmarks. The literature shows that incorporating domain-specific knowledge (through biomedical embeddings or training on clinical corpora) consistently yields better results than purely general models. For example, domain-tuned models like ClinicalBERT outperform general BERT on tasks like i2b2 entity and relation extraction, highlighting the importance of medical context.

\item \textbf{Entity Linking and Knowledge Bases:} We saw that numerous approaches exist for linking entities to medical KBs, from straightforward dictionary methods (e.g., MetaMap's string matching) to advanced neural normalization algorithms. Knowledge bases like UMLS and MeSH provide the essential backbone of medical ontologies, and most works leverage these for standardization. One notable trend is that despite the emergence of sophisticated linking models, many practical systems still rely on or incorporate simple methods due to issues of efficiency and completeness. For instance, even state-of-the-art systems like BERN2 and PubTator (for biomedical text annotation) use hybrid approaches that include dictionary lookups for certain entity types to ensure high coverage. This indicates that purely end-to-end learned linking has not entirely supplanted older techniques, likely because of the challenges we discussed (ambiguity, scale, etc.). This informs our approach to use a proven tool (SciSpacy) and then build on it with custom logic, rather than trying to train a new linker from scratch.

\item \textbf{Knowledge Graph Construction \& Use:} There are multiple reports of building clinical knowledge graphs for different purposes, confirming the feasibility and value of this endeavor. For example, successful cases include constructing a medication interaction KG from electronic prescriptions, or a phenotype-genotype KG from medical literature, and using them to answer complex queries or predict outcomes \cite{Rotmensch2017}. A common theme is that combining data from different sources (EHR notes, structured clinical data, literature, guidelines) into a graph enables insights that siloed data cannot provide. However, each existing work tends to focus on a specific slice (e.g., a KG for cancer patient data, or a KG for pharmacovigilance). There isn't a one-size-fits-all clinical KG, and indeed our thesis is carving out its own niche: generating a knowledge graph from unstructured text with a \textbf{multi-model pipeline} and comparing general vs specialized models within that.
\end{itemize}

After surveying the literature, we identify two key gaps that motivate our research:

1. \textbf{Lack of a Unified Multi-Model Pipeline Evaluation:} Many studies address one piece of the puzzle (NER, linking, RE, or KG construction) in isolation. For instance, a paper might present a new NER model, or an improved relation classifier, evaluated on its own. In building an end-to-end system that goes from raw text to a knowledge graph, one often has to stitch together multiple components, and the interactions between those components can be complex. There is a need for exploring a \textbf{combined approach} where each task is handled by a model best suited for it (a "multi-model" approach), and evaluating the pipeline as a whole on real clinical text. Our thesis addresses this by integrating a domain-specific NER (GLiNER or similar) with a proven linker (SciSpacy/UMLS) and a large language model for relation extraction, thereby leveraging strengths of each. To our knowledge, prior work has not thoroughly documented or evaluated such a pipeline. For example, while one study might use SciSpacy for NER+linking and another might use BERT for end-to-end extraction, we are explicitly combining specialist tools (which is often how a practical system would be built) and will analyze how they complement or hinder each other. This includes how errors propagate (e.g., if NER misses an entity, no relation can be extracted for it) and how using multiple threads (parallel processing) impacts throughput. By fully \textbf{revising and integrating} the pieces, our work aims to provide a blueprint and assessment for multi-model information extraction pipelines in healthcare, which is an area not comprehensively covered in literature.

2. \textbf{Limited Comparative Analysis of General vs. Domain-Specific Models:} Another gap is the understanding of trade-offs between general NLP models and domain-specific models in the context of clinical IE tasks. While it's generally accepted that domain-tuned models outperform on in-domain evaluations, this has usually been shown on individual tasks like NER or classification. With the advent of very large general models (e.g., GPT-4) that have surprisingly strong capabilities even on biomedical text, the question arises: could a general model fine-tuned or prompted appropriately perform as well as (or even better than) a domain-specific model on clinical relation extraction and knowledge graph construction? And what about the practical considerations such as speed, cost, or data privacy? The literature so far lacks direct head-to-head comparisons in a full information extraction context. We intend to fill this gap by comparing "MedGemma3-4b" (a model from Google finetuned on de-identified medical text and images) versus "Gemma3-4b" (an instruction tuned version of the base model) on the task of extracting relationships that will form our knowledge graph. This comparative study will shed light on questions like: Does the medical model truly capture subtle clinical context better (e.g., understanding negations or the significance of certain lab findings), or can the general model make up for it with sheer parametric knowledge? Are there particular relation types where one class of model excels over the other? For instance, perhaps the general model, having seen open-text, might better handle unconventional phrasing, whereas the medical model might do better on medication dosages and specific jargon. No prior study, to our knowledge, has provided a systematic evaluation of this within one pipeline, which is what Chapter 4 of this thesis contributes.

In addition to these two major gaps, our work also pushes into some relatively under-explored areas such as the practical performance of running such pipelines on modern hardware (e.g., how to optimize a multi-model pipeline on limited memory Apple Silicon, as noted in Chapter 5) which we consider important for real-world adoption.

By addressing the above gaps, the thesis aims to advance the state-of-the-art in transforming unstructured clinical text into actionable knowledge graphs. We not only propose a novel pipeline architecture but also critically evaluate the components and alternatives. The end result will be a clearer understanding of how to best combine tools and models for clinical information extraction, and evidence-based insight into the value of domain-specific versus general AI models in this setting. This bridges the isolated advances in literature into an integrated solution, moving a step closer to practical deployment in healthcare environments.
