% Chapter 2

\chapter{Literature Review and Theoretical Background}
\label{chap:literature}

\section{Natural Language Processing in Biomedicine}

\subsection{Evolution of Biomedical NLP}

Natural language processing (NLP) techniques have been applied to biomedical text for several decades, encompassing both clinical documentation and scientific literature. Early biomedical NLP systems relied on \textbf{rule-based} or \textbf{dictionary-driven} approaches using handcrafted rules and medical term lookup lists. These systems, including tools like cTAKES, MetaMap, and PubTator, achieved high precision on well-defined patterns but were brittle with limited coverage and labor-intensive maintenance for evolving biomedical language across different domains (clinical notes, research abstracts, experimental protocols).

By the late 1990s and 2000s, \textbf{statistical machine learning} approaches emerged in biomedical NLP. Researchers formulated problems like entity recognition as supervised learning tasks using classification algorithms (SVMs, Maximum Entropy) trained on annotated biomedical corpora from both clinical and research domains. Sequence labeling methods like Hidden Markov Models and Conditional Random Fields became popular for \textbf{biomedical named entity recognition}, using probabilities rather than fixed rules but still requiring careful feature engineering by experts.

Over the last decade, the field has undergone a \textbf{deep learning revolution}. Neural network–based models automatically learn features from large text amounts, achieving state-of-the-art results. Initial approaches used \textbf{recurrent} and \textbf{convolutional neural networks}, while \textbf{word embeddings} boosted performance by leveraging semantic similarities. The \textbf{transformer architecture} and BERT-like models brought another leap, enabling fine-tuning on biomedical datasets with excellent results across clinical notes, research abstracts, and experimental reports. This evolution progressed from rigid rule-based systems to statistical ML and now end-to-end deep learning, dramatically improving biomedical text processing despite remaining challenges.

\subsection{Current State-of-the-Art Approaches}

State-of-the-art (SOTA) NLP in biomedicine today is dominated by \textbf{advanced neural models}, especially transformer-based language models. Researchers routinely use \textbf{domain-specific pretrained models} like \textbf{BioBERT}, \textbf{ClinicalBERT}, and \textbf{SciBERT}, which continue pretraining BERT on biomedical literature or clinical corpora. These models learn medical terminology nuances and significantly outperform generic NLP models on tasks like biomedical NER, relation extraction, and question answering across both clinical and research text, making biomedical transformers the de facto starting point for biomedical NLP studies.

In parallel, \textbf{large language models (LLMs)} like GPT-3, GPT-4, and PaLM are beginning to influence biomedical NLP. These models demonstrate impressive capabilities in \textbf{zero-shot or few-shot settings}, such as parsing research abstracts, extracting gene-disease associations, or classifying drug-adverse effect relationships through prompting alone. While not specialized to biomedical text, their scale often compensates, and it is now state-of-the-art to explore \emph{hybrid approaches} using domain-specific models for certain tasks and general LLMs for others.

Despite excitement around transformers and LLMs, practical state-of-the-art systems combine multiple components. Top-performing pipelines might use \textbf{biomedical NER models}, \textbf{rule-based methods} for specific patterns, and \textbf{transformer-based relation extractors}, often integrating with biomedical knowledge bases like UMLS. Current best approaches leverage (1) pretrained biomedical models, (2) large general models for zero-shot capabilities, and (3) domain knowledge to handle biomedical data idiosyncrasies across clinical and research contexts. This thesis builds upon these trends, employing specialized models and knowledge resources to maximize biomedical text extraction performance.

\section{Named Entity Recognition in Biomedical Text}

\subsection{Traditional NER Methods}

\textbf{Named Entity Recognition (NER)} in biomedical text involves identifying spans corresponding to biomedical concepts like genes, diseases, chemical compounds, species, or clinical procedures. Traditional approaches were dominated by \textbf{rule-based and dictionary-based methods}. Dictionary-based methods compile extensive entity lists for each category and scan text for exact or fuzzy matches across both clinical notes and research literature. Rule-based methods rely on human-crafted linguistic patterns, such as "if 'associated with' appears, following words until punctuation are likely Disease entities." Tools like \textbf{cTAKES}, \textbf{MetaMap}, and \textbf{PubTator} exemplify these approaches, using UMLS dictionary lookups and lexical variation generation for different biomedical text types.

These traditional methods achieved \textbf{high precision} for entities matching their lists or patterns but showed \textbf{limited recall and poor adaptability}. New terminology or unexpected phrasing would be missed, and rule-based systems struggled with biomedical language variability across different text types (clinical notes vs. research abstracts). Maintaining these systems was difficult due to evolving medical vocabulary, varying institutional language use, and differences between clinical and research terminology. \textbf{Ambiguity} posed another challenge – terms like "cold" could refer to common cold (disease) or cold temperature (symptom) without complex understanding.

Traditional NER methods remain valuable in constrained scenarios due to their \textbf{interpretability} and quick deployment with decent lexicons. They complement modern methods as fallback components, using precision to validate neural model outputs. However, the field has largely moved beyond purely rule-based NER due to labor requirements and the need for higher recall and robustness.

\subsection{Deep Learning Approaches for Biomedical NER}

Deep learning approaches revolutionized biomedical NER by removing manual feature engineering and vastly improving generalization. Around the mid-2010s, researchers began applying \textbf{neural network models} like \textbf{BiLSTM-CRF} to biomedical NER tasks across clinical and research text. These models capture bidirectional context through LSTM and ensure consistent label sequences via CRF layers, quickly outperforming earlier hand-crafted feature models. Studies found that incorporating \textbf{character-level CNN or LSTM} sub-networks further improved recognition of rare or misspelled entities by capturing morphological patterns common in biomedical terminology.

A key advantage of deep learning NER is \textbf{word embeddings}. Biomedical terms like "aspirin" and "ibuprofen" have similar vector representations when trained on biomedical data, capturing their shared medication category. \textbf{Domain-specific embeddings} from biomedical corpora (e.g., PubMed articles, clinical notes) outperformed general embeddings, allowing models to leverage vast unlabeled text and recognize similar entities even when absent from training data.

By the late 2010s, \textbf{transformer-based models} took center stage. \textbf{BERT} introduced powerful contextual representation, with researchers fine-tuning pretrained BERT variants like BioBERT on labeled biomedical NER datasets. This approach achieves state-of-the-art performance, with transformers substantially outperforming earlier systems on benchmarks like the i2b2/VA challenge and BioCreative tasks. Deep models excel at handling \textbf{varied phrasing and contexts}, learning entity identification from context rather than explicit formatting across different biomedical text types.

Modern NER trends include \textbf{multiple corpora and multi-task learning} for improved generalization. Systems like \textbf{HunFlair} combine diverse biomedical datasets to create robust models that don't overfit to single text styles \parencite{Weber2023}. \emph{Zero-shot} approaches like \textbf{GLiNER} attempt entity recognition without dataset-specific training, though they typically trail well-trained supervised models in accuracy.

Deep learning has become the dominant biomedical NER paradigm due to superior performance. The combination of powerful architectures, unsupervised pretraining, and larger datasets has pushed concept extraction to new heights. While challenges remain regarding labeled datasets and edge case handling, the gap between human and automated NER performance has narrowed significantly in the biomedical domain.

\subsection{Biomedical Language Models}

A major factor in modern biomedical NLP success is \textbf{biomedical language models} - large neural networks pretrained on biomedical text then fine-tuned for specific tasks. Notable examples include \textbf{BioBERT}, \textbf{ClinicalBERT}, \textbf{SciBERT}, and \textbf{PubMedBERT}. While general models like BERT learn from Wikipedia, they may not understand specialized biomedical vocabulary spanning clinical, research, and pharmaceutical domains. Domain pretraining allows models to pick up medical terminology - BioBERT trained on 4.5 billion PubMed words, while ClinicalBERT used EHR notes to learn clinical shorthand, and SciBERT combined both research abstracts and clinical text.

These biomedical models have unequivocally improved biomedical NLP performance, serving as \textbf{foundational models} requiring little fine-tuning data. BioBERT-based NER models significantly outperform vanilla BERT on biomedical tasks because BioBERT "knows" terms like "bradycardia" and "BRCA1" from pretraining. Studies show BioBERT \textbf{"largely outperforms BERT and previous state-of-the-art models"} on biomedical benchmarks like JNLPBA, ChemProt, and BioRED. These models often include domain-specific tokenizers for better scientific term representation across different biomedical text types.

Another development is \textbf{continual pretraining and task-specific pretraining}, with models like \textbf{BioMegatron} and \textbf{PubMedGPT} trained from scratch on biomedical text. The general finding is that \textbf{domain knowledge leads to better understanding} - models reading research papers better recognize "PD-1 inhibitor" as a drug or "p53 mutation" as a genetic variant compared to general models.

Biomedical language models have limitations: high computational requirements and struggles with rare or new terms. However, they represent the state-of-the-art starting point. This thesis leverages such models expecting enhanced performance, with our \textbf{medical-specific versus general model} comparison essentially evaluating domain pretraining value - a topic of great current research interest.

\section{Entity Linking Techniques}

\subsection{Knowledge Base Linking Methods}

Once named entities are recognized, the next step is \textbf{entity linking} (entity normalization or grounding) - mapping text spans (e.g., "aspirin") to canonical identifiers in a knowledge base. In medical domains, this is crucial for resolving synonyms and variations to standard references, such as linking "heart attack" and "myocardial infarction" to the same concept.

A variety of techniques have been developed for entity linking in biomedicine, ranging from simple string matching to complex machine learning models:

\begin{itemize}
  \item \textbf{String Matching and Dictionary Lookup:} The most straightforward method uses dictionaries of concept names from knowledge bases to match entity mentions. \textbf{Traditional techniques performed recognition and linking in one step}, scanning text for substrings appearing in the KB. Tools like \textbf{MetaMap} and \textbf{cTAKES} allow fuzziness (ignoring case, typos, word order) and rules for common variations. These precision-oriented methods may miss entities without exact matches and struggle with ambiguous mentions having multiple possible mappings.

  \item \textbf{Rule-based and Heuristic Linking:} Some systems add heuristic rules beyond raw dictionary lookup, such as preferring matches whose semantic type fits context (mapping "aspirin" in medication sections to drug concepts). \textbf{Co-occurrence} heuristics use literature patterns to influence selection. Systems like \textbf{GNormPlus} combine dictionary matching with custom post-processing rules, requiring domain expertise and tailoring to specific entity types.

  \item \textbf{Machine Learning-based Linking:} Researchers introduced ML models for linking, formulating it as a \textbf{ranking problem} given candidate concept IDs. \textbf{DNorm} \parencite{Leaman2013} used pairwise learning-to-rank for disease normalization, while \textbf{TaggerOne} \parencite{Leaman2016} employed semi-Markov models for joint recognition and normalization. These approaches require training data but outperform dictionary lookups by learning context-based disambiguation.

  \item \textbf{Neural Embedding-based Linking:} Deep learning approaches embed mentions and KB terms into vector spaces for similarity computation. The idea uses neural encoders to convert mentions and concepts into vectors where correct concepts are closest to mentions. Researchers have used \textbf{character-level BiLSTMs} or CNNs, like \cite{Phan2019} who trained char-BiLSTMs with multiple objectives for mention-concept similarity and context coherence.

  \item \textbf{Pretrained Language Model for Linking:} Latest methods use transformers for entity linking. \textbf{BioSyn} \parencite{Sung2020} uses BioBERT to encode mentions and concept names, fine-tuning so mention vectors are close to true concept names and far from others. At inference, it performs fast nearest-neighbor search among concept embeddings, achieving high accuracy by capturing rich synonym and context understanding.
\end{itemize}

Each of these methods has trade-offs. Simpler methods (string match) are fast and don't require training data, but can't resolve ambiguity well. Advanced methods (neural linking) are more accurate in principle but require significant computation and annotated training data. Moreover, some neural methods struggle with the \textbf{scale} of biomedical KBs performing a vector search over millions of concepts can be slow or memory-intensive, which is why approximate search or pruning strategies are used. In practice, many systems adopt a \textbf{hybrid approach}: e.g., use quick string matching to get candidates, then a learned model to pick the best, combining speed with accuracy. Our work specifically leverages \textbf{SciSpacy's} linking (described below) for candidate generation, which is a strong unsupervised method, and we incorporate confidence scoring and domain rules to refine the results.

\subsection{UMLS and MeSH Knowledge Bases}

In the biomedical domain, the two predominant knowledge bases for entity linking are \textbf{UMLS} and \textbf{MeSH}. Understanding their nature explains why they're favored over general-purpose KBs like Wikipedia for clinical NLP.

The \textbf{Unified Medical Language System (UMLS)} is a comprehensive metathesaurus maintained by the U.S. National Library of Medicine, aggregating over 150 biomedical vocabularies into a unified framework. UMLS provides concept unique identifiers (CUIs) linking concepts to all names, synonyms, and semantic types. For example, aspirin has CUI C0004057 connecting "Aspirin," "Acetylsalicylic Acid," and brand names as a pharmacologic substance. UMLS is \textbf{massive} with over \textbf{3 million concepts} from sources like SNOMED CT, RxNorm, and ICD, enabling integration with electronic health records using standard codes.

\textbf{MeSH (Medical Subject Headings)} is a curated hierarchical taxonomy primarily for indexing PubMed articles, with about \textbf{30,000 main headings} covering diseases, chemicals, and anatomical terms. MeSH terms help librarians tag articles for retrieval and can serve as linking targets for biomedical literature. SciSpacy supports MeSH linking as a UMLS alternative. MeSH is \textbf{smaller and more curated} with well-defined concepts but won't include every clinical term or abbreviation.

Using medical-specific KBs offer clear advantages over general-purpose knowledge bases. \textbf{Coverage}: UMLS likely contains obscure medical acronyms that Wikipedia lacks. \textbf{Terminology consistency}: UMLS provides unified identifiers for concepts with different names across contexts. \textbf{Domain relevance}: UMLS/MeSH have medical-tailored semantic types and treatment relationships, enabling richer graph construction compared to general KBs with unrelated entries.

However, UMLS's comprehensiveness introduces ambiguity with overlapping concepts and names, making linking challenging. UMLS requires licensing (free for research), explaining why specialized tools pre-package the data. Our project predominantly uses \textbf{UMLS} for its broad coverage, occasionally referencing \textbf{MeSH} for alternate approaches.

\subsection{SciSpacy's TF-IDF Character N-gram Matching}

\textbf{SciSpacy} \parencite{Neumann2019} is an open-source library by the Allen Institute for AI that extends spaCy to scientific and biomedical text. It includes pre-trained biomedical NER models and an \textbf{EntityLinker} component for linking text spans to knowledge base entries. Our system leverages SciSpacy for entity linking.

SciSpacy's EntityLinker uses a \textbf{sparse vector model based on TF-IDF over character n-grams}. Each concept name in the knowledge base is indexed by character 3-grams (e.g., "aspirin" becomes "asp", "spi", "pir", "iri", "rin"). When linking entity mentions, SciSpacy computes TF-IDF weighted n-gram vectors and performs approximate nearest neighbor search to find closest matches by cosine similarity. This robust string matching handles minor spelling differences and partial matches, similar to tools like \textbf{QuickUMLS}.

The SciSpacy linker returns candidate KB identifiers with \textbf{similarity scores} for each entity span, typically the top 5 candidates sorted by TF-IDF similarity. For "COPD," it might return "Chronic Obstructive Pulmonary Disease" (score 0.95) and "Compulsive Obsessive Personality Disorder" (score 0.60). Users can set thresholds and retrieve concept names, definitions, etc., for each CUI.

SciSpacy integrates \textbf{abbreviation detection} using the \textbf{AbbreviationDetector} based on Schwartz \& Hearst algorithm. It scans for patterns like ``Full Form (ABBR)'' and learns abbreviation definitions. With \texttt{resolve\_abbreviations=True}, the EntityLinker attempts to link the \emph{long form} rather than short form, greatly improving linking accuracy for abbreviations and acronyms.

SciSpacy's TF-IDF n-gram approach is \emph{fast} and requires no training data, applicable to large KBs through efficient search libraries. However, it purely matches surface forms without using context beyond the mention string. We leverage SciSpacy to get candidate UMLS CUIs, then apply confidence thresholds and custom logic to finalize linking. It provides a solid baseline capturing most cases, especially when mentions closely resemble canonical names.

\subsection{Challenges in Medical Entity Linking}

Linking clinical entities to a knowledge base is a challenging task due to several inherent issues in medical text and the nature of biomedical knowledge bases:

\begin{itemize}
  \item \textbf{Synonymy and Term Variation:} Medical concepts have numerous expressions - "heart attack," "myocardial infarction," "AMI," and "cardiac infarct" all refer to the same concept. Linking systems must recognize these variants and unify them. UMLS provides synonyms, but text might contain unlisted variations like misspellings or specific descriptions. Ensuring different surface forms map to one concept rather than related but different concepts is a \textbf{recall} challenge.

  \item \textbf{Ambiguity (Polysemy):} Many medical terms are ambiguous - "RA" could mean rheumatoid arthritis, right atrium, or renal artery depending on context. When mentions map to multiple KB entries, linkers must pick correctly. This \textbf{precision} challenge requires using context (neighboring words, document metadata), which simple string-based methods don't fully exploit. Context-aware linking remains challenging, especially with limited training data.

  \item \textbf{Data Scarcity for Training:} Unlike NER, linking requires \textbf{mapping to concepts}, a complex annotation task needing domain experts. Creating gold-standard corpora where every mention maps to concepts like CUIs is time-consuming. Limited large training corpora means supervised ML approaches can overfit. This explains why dictionary and heuristic approaches remain popular - they don't need training data.

  \item \textbf{Knowledge Base Coverage and Granularity:} UMLS is huge but imperfect - concepts might be \textbf{missing} or at wrong granularity. For "brittle diabetes" (unstable Type 1), should it map to Type 1 diabetes or general diabetes mellitus? UMLS merges many sources, creating duplicative concepts that confuse linking systems. Choosing appropriate granularity and handling similar concepts requires nuanced understanding.

  \item \textbf{Abbreviations and Shorthand:} Abbreviations are extremely common but detection isn't guaranteed. Some are ambiguous or user-specific - "pt c/o CP" means "patient complains of chest pain." Linking "CP" requires knowing both the expansion and that chest pain is a symptom concept. Multi-step approaches (detect, expand, link) are needed, but failed expansion makes direct linking low-confidence.

  \item \textbf{Context and Relation Constraints:} Linking depends on context - "family history of breast cancer" should link to breast cancer \emph{flagged as family history}, while "denies chest pain" involves negated context. While linking typically ignores negation, some applications want to encode that patients \emph{don't have} problems. This challenge is usually left to pre-processing or post-processing rather than linking algorithms.

  \item \textbf{Scalability and Performance:} High-end methods like BERT encoding every mention against millions of concepts would be extremely slow. Scalability issues mean simpler methods are chosen as compromises. Even approximate nearest neighbor methods require high memory for concept embeddings. Making linking both \emph{fast and accurate} is an active engineering problem - we chose SciSpacy's efficient approach to avoid bottlenecks.
\end{itemize}

Due to these challenges, \textbf{no single linking method is perfect}, and mistakes can propagate to later stages (e.g., linking "Cold" to "Common Cold" when meaning "cold sensation" creates erroneous knowledge graph entries). Our thesis addresses these through confidence scoring, abbreviation handling, and constraining entity types. Many state-of-the-art tools still include \textbf{manual or rule-based components} for certain entities, acknowledging that combined approaches (simple methods for easy cases, complex methods for hard cases) are often the best practical solution – a philosophy we carry into our multi-model system.

\section{From Relationship Extraction to Knowledge Graphs}

\subsection{Relationship Extraction Methods}

Identifying relationships between medical entities is the step that transforms isolated pieces of information into structured knowledge. In text, a \textbf{relationship extraction (RE)} system might detect, for example, that a certain drug is indicated for a disease, or that one clinical finding is a symptom of a condition mentioned elsewhere in the text. Extracting such relations from unstructured text allows us to then build edges in a knowledge graph (connecting the nodes that were identified via NER and linking).

\subsubsection{Rule-Based Approaches}

Early approaches to relation extraction in clinical text were predominantly \textbf{rule-based or pattern-based}, relying on human experts to define relationship patterns. Simple rules might be: \emph{if a medication and condition appear with "for" or "to treat" between them, infer a Treats relation}. Examples include "<Drug> for <Condition>" templates or using trigger verbs ("caused", "due to") and syntactic patterns like dependency parsing to identify relationships.

Rule-based RE offers \textbf{precision-oriented and interpretable} advantages - extracted relations can be traced to specific rules. In critical applications like adverse event identification, carefully crafted rules provide reliability. Rules easily incorporate domain knowledge, such as "if X is a Finding and Y is a Disease connected by 'of', then X is a symptom of Y."

However, \textbf{coverage is a major issue}. Language variability makes it impossible to enumerate all relationship expressions. Pattern-based methods struggle with \textbf{negation and temporal expressions} - "No evidence of metastasis to the lung" contains relation indicators but is negated. This leads to \textbf{low recall}, with methods capturing only subsets of true relations and missing unexpectedly phrased relationships \parencite{Laue2024}.

Despite limitations, rule-based approaches remain useful in certain settings for flagging specific events due to easier maintenance and verification. They complement statistical methods by handling obvious cases with high precision before machine learning processes the rest. Rule-based RE methods provide important groundwork with high precision and transparency but require extensive knowledge engineering and don't scale to real-world text diversity.
\subsubsection{Machine Learning Techniques}

As annotated clinical text datasets became available through shared tasks like i2b2 challenges, \textbf{machine learning (ML)} approaches to relation extraction gained prominence. Rather than manually specifying linguistic patterns, these approaches train models on labeled examples where entities and their relations are annotated, then learn to predict relations in new text.

\textbf{Feature-based ML:} Early ML methods used classifiers like Support Vector Machines or Logistic Regression, requiring text conversion to feature vectors. Researchers engineered features capturing signals like words between entities, entity order and types, syntactic parse features, and section information. For example, determining "adverse reaction" relations might use features like temporal proximity ("Penicillin allergy") or connecting verbs ("causes"). Feature-based ML was successful in competitions, with many top i2b2 entries using SVMs with cleverly designed features, substantially beating pattern-matching in recall while maintaining precision.

One limitation is these models \textbf{don't generalize well to contexts not represented in features}. If important cues aren't captured as features, models can't learn from them. They also required extensive domain-specific tweaking, essentially moving the burden from writing rules to writing good features.

\textbf{Neural Network ML:} Relation extraction moved toward neural approaches around mid-2010s. Models like \textbf{CNNs} and \textbf{RNNs} were applied to word sequences between target entities. Classic architectures represent sentences with marked entity positions as embedding sequences, feed into networks, and output relation classes. More recently, transformer models like BERT achieve state-of-the-art results by inserting special tokens around entities and using the output for relation classification.

ML approaches (especially neural ones) greatly improved \textbf{recall and adaptability}, finding subtly expressed relations that rules might miss without requiring human pattern enumeration. However, deep models need substantial labeled data, challenging in clinical domains. Many models use combined datasets or synthetic data supplementation. \textbf{Error interpretability} remains difficult when learned models make mistakes.

The current trend favors neural RE when data is available, consistently outperforming feature-based methods. Hybrid systems combine neural outputs with rule-based frameworks for optimal results. In our project, we explore using large language models for \textbf{relationship extraction}, taking the neural approach to extremes by leveraging models trained on enormous general text corpora.

\subsubsection{Large Language Models for Relationships}

The latest frontier in relationship extraction uses \textbf{Large Language Models (LLMs)} like GPT-3, GPT-4, and PaLM. These models, trained on diverse internet-scale text, perform tasks via prompting without explicit task-specific training through general language understanding. In biomedical relation extraction, researchers investigate LLM capabilities for identifying relationships without training. Findings are promising: \textbf{"Large Language Models have demonstrated impressive performance in biomedical relation extraction, even in zero-shot scenarios."} \cite{Laskar2025}.

There are a couple of ways LLMs are used for RE:

\begin{itemize}
\item \textbf{Prompt-based Classification:} You can present LLMs with prompts describing the relation extraction task and specific instance. For example, to classify gene-disease relations, prompt GPT-4 with: \emph{"Determine if there is a positive, negative, or no relation between [BRCA1] and [breast cancer] in: 'The BRCA1 mutation is associated with higher breast cancer risk.' Answer '1' for positive, '0' for no relation."}. This turns RE into question-answer format. With carefully engineered prompts, LLMs achieve accuracy comparable to supervised models on benchmarks, requiring \textbf{no training phase} on task-specific data - extremely useful in clinical RE where annotated datasets are scarce.

\item \textbf{Prompt-based Extraction (Generative):} Instead of classifying given options, LLMs can directly extract relations open-endedly. For instance: \emph{"Extract all <drug, condition> treatment pairs from the text..."} or ask for structured JSON output listing problems with associated tests/medications. This leverages LLMs' generative nature for flexible information extraction. Tools like GPT-4 can parse complex text and output triples with surprising accuracy, inherently performing joint NER and RE by identifying entities and relations simultaneously during generation.
\end{itemize}

Using LLMs for RE brings \textbf{new challenges}. First, evaluation is tricky since LLMs might express relationships differently from gold standards, making automatic evaluation difficult. Second, they can \textbf{hallucinate} - producing plausible but unstated relations, like "filling in" likely treatments not explicitly mentioned. Ensuring models stick to text evidence is active research (chain-of-thought prompting, instructions against unmentioned facts). Third, \textbf{privacy and deployment}: powerful LLMs accessible only via cloud APIs are problematic for patient data, spurring interest in smaller domain-specific models for local hospital deployment.

Despite issues, LLMs' zero-shot or few-shot RE ability is potentially game-changing, dramatically reducing development time and adapting to multiple tasks easily. Our thesis compares a \textbf{medical-specific model} (MedGemma3-4b) with a \textbf{general-purpose model} (Gemma3-4b), examining how much general LLMs know about clinical relationships versus medical-tuned models. Prior work suggests general models are strong but domain-tuned models maintain advantages in specialized accuracy. LLMs represent cutting-edge relationship extraction, offering flexibility and massive implicit knowledge, likely playing increasing roles in clinical information extraction systems.

\subsection{Knowledge Graph Construction}

\subsubsection{Graph Database Technologies}

A \textbf{knowledge graph} is essentially a network of entities (nodes) linked by relationships (edges) that represent facts or assertions. Once we extract entities and relationships from text, we need to store them in a structured way for querying and analysis. This is where \textbf{graph database technologies} come into play. Unlike traditional relational databases that use tables, graph databases are designed to store and query data in terms of nodes and edges, which is a natural fit for knowledge graphs.

There are two main paradigms for graph data management: \textbf{RDF triple stores} and \textbf{property graph databases}.

\begin{itemize}
\item \textbf{RDF Triple Stores:} These databases store data as triples (subject, predicate, object) following the Resource Description Framework standard. For example, (Aspirin, treats, Diabetes) would be a triple. Triple stores like Apache Jena TDB, Virtuoso, and GraphDB use SPARQL query language, which resembles SQL but is tailored to graph patterns with variables. RDF systems excel at enforcing schemas/ontologies and integrating data from multiple sources via shared identifiers, making them popular in academic knowledge graph work requiring semantic interoperability.

\item \textbf{Property Graph Databases:} More commonly used in industry, property graph models allow each \textbf{node} to have labels and properties (key-value pairs), while each \textbf{relationship} has a type and properties. For example, a ``Medication'' node might have \texttt{name="Aspirin"} and \texttt{dosage="325mg"}, connected via ``TREATS'' edge with \texttt{source="doctor\_notes"} property. This flexible model allows attaching contextual data directly on nodes/edges. \textbf{Neo4j} leads this category with user-friendly, optimized graph queries, alongside Amazon Neptune, JanusGraph, and TigerGraph.
\end{itemize}

Graph databases allow efficient traversal of the graph. For instance, a query like ``find all medications that treat diseases which have symptom X'' would involve traversing from a symptom node X, to disease nodes connected by ``hasSymptom'' edges, then from those diseases out via ``treatedBy'' or ``treated\_with'' edges to medication nodes. Doing this with SQL would be complex and slow if the data is heavily relational and many joins are needed, whereas graph DBs are optimized for such multi-hop queries.

Moreover, graph databases align with how we conceptualize knowledge: they can directly store an edge saying \textbf{Drug$\rightarrow$treats$\rightarrow$Disease}, which is more intuitive than having to indirectly connect them via foreign keys in tables. In a knowledge graph, relationships are first-class citizens -- you can query them, filter on them (e.g., find all ``inhibits'' relations between genes and proteins, etc.), and even put constraints on paths.

In summary, graph database technologies provide the backbone for \textbf{storing} and \textbf{querying} the knowledge graph that results from our NLP pipeline. They handle the heavy lifting of managing connections at scale. The choice between RDF vs property graph often comes down to use case; we have chosen a property graph approach (Neo4j) for its straightforwardness and the ability to easily attach attributes to nodes/edges which is useful for tracking metadata of extracted info.

\subsubsection{Neo4j and Cypher Query Language}

\textbf{Neo4j} is a widely-used graph database that implements the property graph model and has become a standard for many knowledge graph projects. It's known for its performance, developer-friendly interface, and a rich ecosystem of tools. In the context of our thesis, Neo4j serves as the platform where the extracted knowledge graph is built and stored. Here's why we chose Neo4j and how we use it:

\begin{itemize}
\item \textbf{Data Model:} In Neo4j, data is stored as nodes and relationships with labels like \texttt{\string:Person}, \texttt{\string:Condition}, \texttt{\string:Drug} for nodes and relationship types like \texttt{DIAGNOSED\_WITH}, \texttt{TREATS} for edges. Each can have properties -- patient nodes link to condition nodes via \texttt{HAS\_CONDITION} edges with \texttt{onset\_date} or \texttt{status} properties. Our extracted graph has nodes representing medical concepts and edges representing text relations. Neo4j's property graph capability allows storing confidence scores or context as edge properties for later analysis.

\item \textbf{Cypher Query Language:} Neo4j uses \textbf{Cypher}, a declarative language for querying graphs with pattern matching. For example:
\begin{verbatim}
  MATCH (d:Drug {name:"Aspirin"})-[:TREATS]->(c:Disease)
  RETURN c.name;
\end{verbatim}
This finds diseases treated by Aspirin using ASCII-art notation where \texttt{()} denote nodes and \texttt{-[]->} denote relationships. We generate Cypher programmatically, using \texttt{MERGE} to create nodes and relationships without duplicates. Cypher's readability makes graph construction verification easier.

\item \textbf{Advantages of Neo4j:} Neo4j is optimized for \textbf{graph traversals} - queries requiring multiple JOINs in SQL are fast due to index-free adjacency where nodes directly know their connections. Features include full-text indexing and APOC library for graph algorithms. The web-based browser interface allows manual knowledge graph exploration with visual node and relationship plotting, revealing patterns or linking errors.

\item \textbf{Cypher Query Generation:} Our methodology translates extracted information into Cypher queries for each processed clinical note. We batch queries and use transactions for efficient insertion, resulting in a Neo4j database containing the consolidated knowledge graph from all notes.\end{itemize}

In summary, \textbf{Neo4j} provides a robust platform for our knowledge graph, and \textbf{Cypher} allows us to interact with that graph in a flexible way – whether to populate it or to retrieve insights. Many published clinical knowledge graph projects (and industry solutions) use Neo4j due to these strengths \parencite{Zimbres2024}. By using Neo4j, we align with best practices and ensure that our graph is not just a conceptual outcome, but a queryable database that can answer complex questions about the data model we've constructed.

\subsection{Biomedical Knowledge Graph Applications}

Knowledge graphs (KGs) in the biomedical domain unlock a range of powerful applications by enabling connections across different types of medical data and knowledge. By converting unstructured biomedical text into a graph of entities and relationships, we create a structure that can be traversed and analyzed to support clinical care, biomedical research, and drug discovery. Here we review some key applications of biomedical knowledge graphs, as reported in literature and practice \parencite{Milvus2025}:

\begin{itemize}
\item \textbf{Integrating Biomedical Data for Comprehensive Analysis:} Knowledge graphs break down data silos by representing biomedical entities (genes, diseases, drugs, species, pathways) as connected subgraphs. A KG might link gene nodes to disease and drug nodes, enabling complex queries like "Find genes associated with Disease X that are targets of Drug Y." This supports both clinical applications and research hypothesis generation. For clinical use, patient nodes connected to "Type 2 Diabetes" can link to genetic risk factors and guideline nodes. For research, disease-gene-drug connections enable systematic literature analysis and drug repurposing opportunities.

\item \textbf{Knowledge Discovery and Hypothesis Generation:} Knowledge graphs enable systematic exploration of biomedical relationships extracted from literature. KGs can reveal indirect connections like "Gene A → Disease B → Drug C" suggesting potential therapeutic targets. Research applications include identifying novel drug-disease associations, understanding disease mechanisms through pathway analysis, and discovering biomarkers through entity relationship traversal. Graph queries implement reasoning by pattern matching, supporting both clinical decisions and research hypotheses with comprehensive relationship consideration beyond isolated findings.

\item \textbf{Clinical Decision Support (CDS):} Knowledge graphs enhance clinical decision support by encoding medical knowledge and enabling reasoning. KGs can model clinical rules like "Patients with Condition X should receive Test Y every 6 months" as relationships between condition and procedure nodes. Drug→condition→contraindication graphs enable CDS systems to detect conflicts among patient medications. Research prototypes use KGs for diagnosis suggestions by connecting patient symptoms to disease nodes through integrated clinical and research knowledge graphs.
\end{itemize}

\section{Related Work Summary and Research Gap}

In this chapter, we reviewed the literature across the spectrum of our task: from natural language processing in the biomedical domain (entity recognition and linking) to relationship extraction and knowledge graph construction. The existing body of work demonstrates substantial progress in each of these individual areas:

\begin{itemize}
\item \textbf{Biomedical NLP Maturity:} The evolution from rule-based systems to statistical ML and now deep learning/large language models has greatly enhanced biomedical text extraction accuracy across clinical notes and research literature. Robust tools exist for biomedical NER (BiLSTM-CRF, BioBERT-based taggers) and relation extraction, with transformers and prompt-based LLM methods leading benchmarks. Literature consistently shows domain-specific knowledge (biomedical embeddings, clinical and research corpora training) yields better results than general models, with domain-tuned models like BioBERT and SciBERT outperforming general BERT on biomedical tasks.

\item \textbf{Entity Linking and Knowledge Bases:} Numerous approaches exist for linking entities to biomedical KBs, from dictionary methods (MetaMap, PubTator) to advanced neural algorithms. UMLS and MeSH provide essential medical ontology backbones for standardization across clinical and research domains. Despite sophisticated linking models, many practical systems still incorporate simple methods for efficiency and completeness - even state-of-the-art systems like BERN2 and PubTator use hybrid approaches with dictionary lookups. This informs our approach to use proven tools (SciSpacy) with custom logic rather than training new linkers.

\item \textbf{Knowledge Graph Construction \& Use:} Multiple reports confirm biomedical knowledge graph feasibility and value, including drug interaction KGs from literature, gene-disease KGs from research papers, and integrated clinical KGs for outcome prediction \parencite{Rotmensch2017}. Combining diverse data sources (research literature, clinical notes, structured databases, guidelines) enables insights impossible with siloed data. However, existing works focus on specific domains rather than comprehensive solutions. Our thesis carves a niche: generating knowledge graphs from unstructured biomedical text with \textbf{multi-model pipelines} comparing general versus specialized models.
\end{itemize}

After surveying the literature, we identify two key gaps that motivate our research:

1. \textbf{Lack of a Unified Multi-Model Pipeline Evaluation:} Many studies address individual components (NER, linking, RE, or KG construction) in isolation rather than end-to-end systems. There is a need for exploring \textbf{combined approaches} where each task uses the best-suited model, evaluating the complete pipeline on real biomedical text. Our thesis addresses this by integrating domain-specific NER (GLiNER), proven linking (SciSpacy/UMLS), and large language models for relation extraction. We explicitly combine specialist tools and analyze how they complement or hinder each other, including error propagation and parallel processing impacts. Our work aims to provide a blueprint for multi-model information extraction pipelines in biomedicine - an area not comprehensively covered in literature.

2. \textbf{Limited Comparative Analysis of General vs. Domain-Specific Models:} Another gap is understanding trade-offs between general and domain-specific models in biomedical IE tasks. While domain-tuned models typically outperform on individual tasks, very large general models (e.g., GPT-4) show surprisingly strong biomedical capabilities. Could appropriately prompted general models perform as well as domain-specific models on biomedical relation extraction? Literature lacks direct head-to-head comparisons in full information extraction contexts. We fill this gap by comparing "MedGemma3-4b" (Google's medical-finetuned model) versus "Gemma3-4b" (an instruction tuned version of the base model) on relationship extraction for knowledge graph construction. This study examines whether medical models better capture biomedical context or if general models compensate with parametric knowledge, and identifies relation types where each excels - providing systematic evaluation within one pipeline.

By addressing the above gaps, the thesis aims to advance the state-of-the-art in transforming unstructured biomedical text into actionable knowledge graphs. We not only propose a novel pipeline architecture but also critically evaluate the components and alternatives. The end result will be a clearer understanding of how to best combine tools and models for biomedical information extraction, and evidence-based insight into the value of domain-specific versus general AI models in this setting. This bridges the isolated advances in literature into an integrated solution, moving a step closer to practical deployment in biomedical research and healthcare environments.
